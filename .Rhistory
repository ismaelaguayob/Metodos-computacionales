shape = "circle",        # Forma circular para compactar la nube
ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
# Esto nos sirve sobre todo para saber si se nos coló algún bloque de código
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
unnest_tokens(word, post_content)
# Cantidad de palabras extraídas
nrow(words)
# Cargar palabras comunes en español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "2", "real", "realidad"))
# Filtramos las stop words del texto
words_clean <- words %>%
anti_join(stop_words, by = "word") %>%
anti_join(stop_words_es, by = "word")
# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
count(word, sort = TRUE)
# Ver las 10 palabras más frecuentes
head(word_counts, 10)
# Graficamos las palabras más frecuentes
word_counts %>%
filter(n > 10) %>%
slice_max(n, n = 15) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col() +
coord_flip() +
labs(title = "Frecuencia de Palabras en Artículos",
x = "Palabra", y = "Frecuencia") +
theme_minimal()
word_counts_filtered <- word_counts %>% filter(n > 200) %>% slice_max(n, n = 70)
wordcloud2(
data = word_counts_filtered,
size = 0.3,              # Aumenta el tamaño general de las palabras
minSize = 0,             # Asegura que todas las palabras sean visibles
gridSize = 1,            # Ajusta la densidad de palabras
color = "random-dark",   # Colores para las palabras
backgroundColor = "white", # Fondo blanco
shape = "circle",        # Forma circular para compactar la nube
ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
View(word_counts)
# Esto nos sirve sobre todo para saber si se nos coló algún bloque de código
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
unnest_tokens(word, post_content)
# Cantidad de palabras extraídas
nrow(words)
# Cargar palabras comunes en español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "2", "real", "realidad", "había"))
# Filtramos las stop words del texto
words_clean <- words %>%
anti_join(stop_words, by = "word") %>%
anti_join(stop_words_es, by = "word")
# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
count(word, sort = TRUE)
# Ver las 10 palabras más frecuentes
head(word_counts, 10)
# Graficamos las palabras más frecuentes
word_counts %>%
filter(n > 10) %>%
slice_max(n, n = 15) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col() +
coord_flip() +
labs(title = "Frecuencia de Palabras en Artículos",
x = "Palabra", y = "Frecuencia") +
theme_minimal()
word_counts_filtered <- word_counts %>% filter(n > 200) %>% slice_max(n, n = 70)
wordcloud2(
data = word_counts_filtered,
size = 0.3,              # Aumenta el tamaño general de las palabras
minSize = 0,             # Asegura que todas las palabras sean visibles
gridSize = 1,            # Ajusta la densidad de palabras
color = "random-dark",   # Colores para las palabras
backgroundColor = "white", # Fondo blanco
shape = "circle",        # Forma circular para compactar la nube
ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
View(word_counts_filtered)
# Esto nos sirve sobre todo para saber si se nos coló algún bloque de código
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
unnest_tokens(word, post_content)
# Cantidad de palabras extraídas
nrow(words)
# Cargar palabras comunes en español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "2", "real", "realidad", "había"))
# Filtramos las stop words del texto
words_clean <- words %>%
anti_join(stop_words, by = "word") %>%
anti_join(stop_words_es, by = "word")
# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
count(word, sort = TRUE)
# Ver las 10 palabras más frecuentes
head(word_counts, 10)
# Realizamos nube de palabras ,ás frecuentes
word_counts_filtered <- word_counts %>% filter(n > 200) %>% slice_max(n, n = 70)
wordcloud2(
data = word_counts_filtered,
size = 0.3,              # Aumenta el tamaño general de las palabras
minSize = 0,             # Asegura que todas las palabras sean visibles
gridSize = 1,            # Ajusta la densidad de palabras
color = "random-dark",   # Colores para las palabras
backgroundColor = "white", # Fondo blanco
shape = "circle",        # Forma circular para compactar la nube
ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
View(json_data)
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
total_results <- json_data[["total"]]
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
# Iterar sobre cada fila del data frame
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl("inteligencia artificial", parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
}
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2,
wordcloud2
)
rm(list = ls())
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
total_results <- json_data[["total"]]
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
# Iterar sobre cada fila del data frame
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl("inteligencia artificial", parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
}
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name
)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
for (i in seq_len(nrow(datos_proc))){
# Convertimos el contenido a un objeto HTML para usar rvest
contenido_html <- rvest::read_html(datos_proc$post_content[[i]])
# Eliminamos los divs con la clase 'lee-tambien-bbcl'
contenido_html %>%
html_nodes("div.lee-tambien-bbcl") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Instagram
contenido_html %>%
html_nodes("blockquote.instagram-media") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Twitter
contenido_html %>%
html_nodes("blockquote.twitter-tweet") %>%
xml_remove()
# Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
contenido_texto <- as.character(contenido_html)
contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
# Guardamos el contenido limpio de vuelta en el data frame
datos_proc$post_content[[i]] <- contenido_texto
#Revisamos el contenido HTML resultante (opcional)
# print(as.character(contenido_html))
}
View(all_data)
View(datos_proc)
View(all_data)
View(all_data)
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
total_results <- json_data[["total"]]
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
# Iterar sobre cada fila del data frame
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl("inteligencia artificial", parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
}
datos_proc = all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name, parrafos_filtrados
)
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name, parrafos_filtrados
)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
print(datos_proc$parrafos_filtrados)
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
total_results <- json_data[["total"]]
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
# Iterar sobre cada fila del data frame
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(c("inteligencia artificial", "IA"), parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
}
text <- c("intelgiencia artificial", "ia")
text <- c("intelgiencia artificial", "ia")
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
}
text <- "intelgiencia artificial"|"ia"
text <- "intelgiencia artificial|ia"
text <- "intelgiencia artificial|ia"
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
}
progress <- 0
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
progress <- 0
progress <- progress + 1
print(progress, "aplicados")
}
text <- "intelgiencia artificial|ia"
progress <- 0
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
progress <- progress + 1
cat(progress, "aplicados\n")
}
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name, parrafos_filtrados
)
print(datos_proc$parrafos_filtrados)
all_data$parrafos_filtrados <- NA
text <- "intelgiencia artificial|ia"
text <- "\\binteligencia artificial\\b|\\bIA\\b"
progress <- 0
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
progress <- progress + 1
cat(progress, "aplicados\n")
}
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name, parrafos_filtrados
)
print(datos_proc$parrafos_filtrados)
