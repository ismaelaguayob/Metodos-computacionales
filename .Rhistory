footnote = "Fuente: Elaboración propia con datos de PISA.")
table_1 <- table1::table1(~ effgen + effspec | OCDE, data=pisa22ict, topclass="Rtable1-zebra", caption = "Tabla 1: Estadísticos Descriptivos índice",
footnote = "Fuente: Elaboración propia con datos de PISA.")
table_1 <- table1::table1(~ effgen + effspec | OCDE, data=pisa22ict, topclass="Rtable1-zebra", caption = "Tabla 1: Estadísticos Descriptivos índice",
footnote = "Fuente: Elaboración propia con datos de PISA.")
table_1
tabla_1 <- table1::table1(~ effgen + effspec + ICTEFFIC | OCDE, data=pisa22ict, topclass="Rtable1-zebra", caption = "Tabla 1: Estadísticos Descriptivos índice",
footnote = "Fuente: Elaboración propia con datos de PISA.")
tabla_1
ggplot2::ggplot(tabla_resultados, aes(CargaFactor1, Variable)) +
geom_point() +
geom_point(
color="#fe3057"
)
ggplot2::ggplot(tabla_resultados, aes(CargaFactor2, Variable)) +
geom_point() +
geom_point(
color="#fe3057"
)
tabla_1 <- table1::table1(~ effgen + effspec + ICTEFFIC + OCDE, data=pisa22ict, topclass="Rtable1-zebra", caption = "Tabla 1: Estadísticos Descriptivos índice",
footnote = "Fuente: Elaboración propia con datos de PISA.")
tabla_1
tabla_2 <- table1::table1(~ effgen + effspec + ICTEFFIC | OCDE, data=pisa22ict, topclass="Rtable1-zebra", overall = FALSE, extra.col=list(`P-value`=pvalue), caption = "Tabla 1: Estadísticos Descriptivos índice",
footnote = "Fuente: Elaboración propia con datos de PISA.")
tabla_2
beeswarm::beeswarm(pisa22ict$effgen ~ pisa22ict$OCDE,
horizontal=TRUE,
method="swarm",
col=c("#5f5758", "#fe3057"),
cex=1,
pch=18,
main= "Distribución de Coeficientes Autoeficacia general",
)
beeswarm::beeswarm(pisa22ict$effgen ~ pisa22ict$OCDE,
horizontal=TRUE,
method="swarm",
col=c("#5f5758", "#fe3057"),
cex=1,
pch=18,
main= "Distribución indice autoeficacia general",
)
beeswarm::beeswarm(pisa22ict$effgen ~ pisa22ict$OCDE,
horizontal=TRUE,
method="swarm",
col=c("#5f5758", "#fe3057"),
cex=1,
pch=18,
main= "Distribución indice autoeficacia general"
)
cat(" - Procesados - ", offset, " - de -", total_resultas)
cat(" - Procesados - ", offset, " - de -", total_results)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2,
wordcloud2,
arrow
)
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2,
wordcloud2,
arrow
)
rm(list = ls())
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
total_results <- json_data[["total"]]
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
all_data$parrafos_filtrados <- NA
text <- "\\binteligencia artificial\\b|\\bIA\\b"
progress <- 0
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
progress <- progress + 1
cat(progress, "aplicados\n")
}
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name, parrafos_filtrados
)
print(datos_proc$parrafos_filtrados)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2,
wordcloud2,
arrow
)
rm(list = ls())
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
total_results <- json_data[["total"]]
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
all_data$parrafos_filtrados <- NA
text <- "\\binteligencia artificial\\b|\\bIA\\b"
progress <- 0
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
progress <- progress + 1
cat(progress, "aplicados\n")
}
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name, parrafos_filtrados
)
print(datos_proc$parrafos_filtrados)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
for (i in seq_len(nrow(datos_proc))){
# Convertimos el contenido a un objeto HTML para usar rvest
contenido_html <- rvest::read_html(datos_proc$post_content[[i]])
# Eliminamos los divs con la clase 'lee-tambien-bbcl'
contenido_html %>%
html_nodes("div.lee-tambien-bbcl") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Instagram
contenido_html %>%
html_nodes("blockquote.instagram-media") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Twitter
contenido_html %>%
html_nodes("blockquote.twitter-tweet") %>%
xml_remove()
# Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
contenido_texto <- as.character(contenido_html)
contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
# Guardamos el contenido limpio de vuelta en el data frame
datos_proc$post_content[[i]] <- contenido_texto
#Revisamos el contenido HTML resultante (opcional)
# print(as.character(contenido_html))
}
datos_proc <- datos_proc %>%
filter(grepl("inteligencia artificial", post_content, ignore.case = TRUE))
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
View(datos_proc)
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html
# Procesar el HTML y extraer el texto
while (contador <= total_results) {
# Convertir a nodo HTML
html_content[[contador]] <- read_html(datos_proc$post_content[[contador]])
# Extraemos y limpiamos el texto
datos_proc$post_content[[contador]] <- html_content[[contador]] %>%
html_text2() %>%
str_squish()
contador <- contador + 1
}
# Esto nos sirve sobre todo para saber si se nos coló algún bloque de código
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
unnest_tokens(word, post_content)
# Cantidad de palabras extraídas
nrow(words)
# Cargar palabras comunes en español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "2", "real", "realidad", "había"))
# Filtramos las stop words del texto
words_clean <- words %>%
anti_join(stop_words, by = "word") %>%
anti_join(stop_words_es, by = "word")
# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
count(word, sort = TRUE)
# Ver las 10 palabras más frecuentes
head(word_counts, 10)
# Realizamos nube de palabras ,ás frecuentes
word_counts_filtered <- word_counts %>% filter(n > 200) %>% slice_max(n, n = 70)
wordcloud2(
data = word_counts_filtered,
size = 0.3,              # Aumenta el tamaño general de las palabras
minSize = 0,             # Asegura que todas las palabras sean visibles
gridSize = 1,            # Ajusta la densidad de palabras
color = "random-dark",   # Colores para las palabras
backgroundColor = "white", # Fondo blanco
shape = "circle",        # Forma circular para compactar la nube
ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
# Agrupamos los datos por año y mes, y contar las publicaciones
publicaciones_por_mes <- datos_proc %>%
group_by(year, month) %>%
summarise(cantidad = n(), .groups = 'drop') %>%
mutate(fecha = as.Date(paste(year, month, "01", sep = "-"))) # Crear una fecha para el eje x
# Creamos el gráfico
ggplot(publicaciones_por_mes, aes(x = fecha, y = cantidad)) +
geom_line(color = "blue", linewidth = 1) + # Línea de publicaciones
geom_point(color = "red", size = 1) + # Puntos en cada mes
geom_smooth(method = "loess", color = "green", se = FALSE, linewidth = 1) + # Curva de tendencia
labs(title = "Cantidad de Notas sobre Inteligencia Artificial Publicadas por Mes",
x = "Año",
y = "Cantidad de Notas") +
theme_minimal() +
scale_x_date(date_labels = "%Y", date_breaks = "1 year") # Mostrar solo los años en el eje x
# Contamos las publicaciones por categoría
publicaciones_por_categoria <- datos_proc %>%
group_by(post_category_primary.name) %>%
summarise(cantidad = n(), .groups = 'drop') %>%
arrange(desc(cantidad))
# Filtramos para quedarnos con las 15 categorías más usadas
top_15_categorias <- publicaciones_por_categoria %>%
top_n(15, cantidad)
# Gráfico de Barras para las 15 categorías más usadas
ggplot(top_15_categorias, aes(x = reorder(post_category_primary.name, -cantidad), y = cantidad, fill = post_category_primary.name)) +
geom_bar(stat = "identity") +
labs(title = "Top 15 Categorías más Usadas en Notas",
x = "Categoría",
y = "Cantidad de Notas") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none")
arrow::write_parquet(datos_proc)
arrow::write_parquet(datos_proc, "data.parquet")
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2,
wordcloud2,
arrow
)
rm(list = ls())
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
total_results <- json_data[["total"]]
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
all_data$parrafos_filtrados <- NA
text <- "\\binteligencia artificial\\b|\\bIA\\b"
progress <- 0
for (i in seq_len(nrow(all_data))) {
# Convertir el contenido HTML en un nodo HTML
nodo_html <- read_html(all_data$post_content[i])
# Extraer los párrafos del nodo HTML
parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
# Filtrar los párrafos que contienen "inteligencia artificial"
parrafos_filtrados <- parrafos[grepl(text, parrafos, ignore.case = TRUE)]
# Guardar los párrafos únicos en la columna correspondiente
all_data$parrafos_filtrados[i] <- list(unique(parrafos_filtrados))
progress <- progress + 1
cat(progress, "aplicados\n")
}
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name, parrafos_filtrados
)
print(datos_proc$parrafos_filtrados)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
for (i in seq_len(nrow(datos_proc))){
# Convertimos el contenido a un objeto HTML para usar rvest
contenido_html <- rvest::read_html(datos_proc$post_content[[i]])
# Eliminamos los divs con la clase 'lee-tambien-bbcl'
contenido_html %>%
html_nodes("div.lee-tambien-bbcl") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Instagram
contenido_html %>%
html_nodes("blockquote.instagram-media") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Twitter
contenido_html %>%
html_nodes("blockquote.twitter-tweet") %>%
xml_remove()
# Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
contenido_texto <- as.character(contenido_html)
contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
# Guardamos el contenido limpio de vuelta en el data frame
datos_proc$post_content[[i]] <- contenido_texto
#Revisamos el contenido HTML resultante (opcional)
# print(as.character(contenido_html))
}
datos_proc <- datos_proc %>%
filter(grepl("inteligencia artificial", post_content, ignore.case = TRUE))
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"
# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(datos_proc$post_content[sample(1:total_results, 1)]), con = out_file)
# Mensaje de confirmación
cat("El contenido en bruto se ha exportado a", out_file)
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html
# Procesar el HTML y extraer el texto
while (contador <= total_results) {
# Convertir a nodo HTML
html_content[[contador]] <- read_html(datos_proc$post_content[[contador]])
# Extraemos y limpiamos el texto
datos_proc$post_content[[contador]] <- html_content[[contador]] %>%
html_text2() %>%
str_squish()
contador <- contador + 1
}
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2,
wordcloud2,
arrow
)
