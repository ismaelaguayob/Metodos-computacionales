while (offset < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2
)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
View(all_data_emol)
View(json_data1)
View(all_data_emol)
View(json_data1)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "aplication", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "aplication", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
data1 <- content(response, "parsed", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "parsed", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
data1 <- content(response, "raw", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "raw", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
data1 <- content(response, "text", encoding = "UTF-8", txt=) # Transformamos el cuerpo de "response" en texto en formato UTF-8
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
View(response)
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
View(response)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
View(json_data1)
View(json_data1[["hits"]][["hits"]])
View((json_data1[["hits"]][["hits"]])[[29]][[1]])
View((json_data1[["hits"]][["hits"]])[[29]][[6]])
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
View(json_data1)
View(json_data1[["hits"]][["hits"]][["_source"]])
View(json_data1[["hits"]][["hits"]][["_source"]])
View(json_data1[["hits"]][["hits"]])
json_data1[["hits"]][["hits"]][["_source"]][["texto"]]
View(json_data1[["hits"]][["hits"]])
json_data1[["hits"]][["hits"]][["_source"]][["autor"]]
json_data1[["hits"]][["hits"]][["_source"]][["texto"]]
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 60 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
all_data_emol <- bind_rows(all_data_emol, json_data1)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- select(json_data1[["hits"]][["hits"]][["_source"]])
all_data_emol <- bind_rows(all_data_emol, data2)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
textoemol <- all_data_emol %>%
select(texto)
View(all_data_emol)
View(json_data1)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- select(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- bind_rows(all_data_emol, data2)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- select(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- bind_rows(all_data_emol, data2)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data_emol <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido
while (from < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
# Aumenta el offset
from <- from + 10
# Realiza la solicitud
response <- GET(url)
# Convierte el cuerpo de respuesta a texto
data1 <- content(response, "text", encoding = "UTF-8")
# Convierte el texto JSON en una lista
json_data1 <- fromJSON(data1, flatten = FALSE)
# Extrae el texto de cada artículo
data2 <- lapply(json_data1[["hits"]][["hits"]], function(x) x[["_source"]][["texto"]])
# Convierte la lista a un data frame
data2 <- data.frame(texto = unlist(data2))
# Une al data frame acumulativo
all_data_emol <- bind_rows(all_data_emol, data2)
# Muestra el progreso
cat("Procesados", dim(all_data_emol), "\n")
}
from <- 0  # Seteamos en 0 para que comience por el primer artículo
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- bind_rows(all_data_emol, data2)
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
}
combine_lists <- function(...) {
# Toma múltiples listas como argumentos y las combina en una sola
combined_list <- c(...)
return(combined_list)
}
## Iteramos hasta que el offset sea menor al total de resultados
response <- GET(url)
from <- 0  # Seteamos en 0 para que comience por el primer artículo
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
all_data_emol <- combine_list(all_data_emol, data2)
}
combine_lists <- function(...) {
# Toma múltiples listas como argumentos y las combina en una sola
combined_list <- c(...)
return(combined_list)
}
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
all_data_emol <- combine_list(all_data_emol, data2)
}
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
cat("Procesados", dim(all_data_emol)) #Mostrar progreso
all_data_emol <- combine_lists(all_data_emol, data2)
}
View(all_data_emol)
## Iteramos hasta que el offset sea menor al total de resultados
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- combine_lists(all_data_emol, data2)
procesados <- procesados +1
print("procesados")
}
from <- 0  # Seteamos en 0 para que comience por el primer artículo
all_data_emol <- list() # Creamos data.frame vacío para almacenar todo el contenido
## Iteramos hasta que el offset sea menor al total de resultados
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- combine_lists(all_data_emol, data2)
procesados <- procesados +1
print(procesados)
}
procesados <- 0
## Iteramos hasta que el offset sea menor al total de resultados
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- combine_lists(all_data_emol, data2)
procesados <- procesados +1
print(procesados)
}
procesados <- 0
all_data_emol <- list() # Creamos data.frame vacío para almacenar todo el contenido
## Iteramos hasta que el offset sea menor al total de resultados
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- combine_lists(all_data_emol, data2)
procesados <- procesados +10
print(procesados)
}
from <- 0  # Seteamos en 0 para que comience por el primer artículo
procesados <- 0
all_data_emol <- list() # Creamos data.frame vacío para almacenar todo el contenido
## Iteramos hasta que el offset sea menor al total de resultados
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el offset para cada iteración (después de que se construya el link)
response <- GET(url) # Realizamos la solicitud
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]])
all_data_emol <- combine_lists(all_data_emol, data2)
procesados <- procesados +10
print(procesados)
}
View(all_data_emol)
all_data_emol[[99]]
# Paso 1: Convertir la lista en un data frame
textos_df <- tibble(texto = unlist(all_data_emol))
View(textos_df)
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE) %>%
na.rm
# Paso 1: Convertir la lista en un data frame
textos_df <- tibble(texto = unlist(all_data_emol), na.omit=TRUE)
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE) %>%
# Cuenta las palabras y las ordena
# Paso 3: Filtrar palabras comunes y sin relevancia
# (opcional, puedes modificar el filtro según tu necesidad)
palabras_irrelevantes <- c("la", "es", "de", "el", "una", "tiene")
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE) %>%
# Cuenta las palabras y las ordena
# Paso 3: Filtrar palabras comunes y sin relevancia
# (opcional, puedes modificar el filtro según tu necesidad)
palabras_irrelevantes <- c("la", "es", "de", "el", "una", "tiene")
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE) %>%
# Cuenta las palabras y las ordena
# Paso 3: Filtrar palabras comunes y sin relevancia
# (opcional, puedes modificar el filtro según tu necesidad)
palabras_irrelevantes <- c("la", "es", "de", "el", "una", "tiene")
frecuencia_palabras <- frecuencia_palabras %>%
filter(!palabra %in% palabras_irrelevantes)
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE) %>%
frecuencia_palabras <- frecuencia_palabras %>%
filter(!palabra %in% palabras_irrelevantes)
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE) %>%
frecuencia_palabras <- frecuencia_palabras %>%
filter(!palabra %in% palabras_irrelevantes)
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE)
# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
count(palabra, sort = TRUE)
frecuencia_palabras <- frecuencia_palabras %>%
filter(!palabra %in% palabras_irrelevantes)
# Paso 4: Graficar las frecuencias de las palabras más comunes
ggplot(frecuencia_palabras %>% top_n(10, n), aes(x = reorder(palabra, n), y = n)) +
geom_col(fill = "skyblue") +
coord_flip() +
labs(title = "Frecuencia de Palabras en Textos",
x = "Palabra",
y = "Frecuencia") +
theme_minimal()
View(frecuencia_palabras)
View(frecuencia_palabras)
