`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html
# Procesar el HTML y extraer el texto
while (contador <= total_results) {
# Convertir a nodo HTML
html_content[[contador]] <- read_html(datos_proc$post_content[[contador]])
# Extraemos y limpiamos el texto
datos_proc$post_content[[contador]] <- html_content[[contador]] %>%
html_text2() %>%
str_squish()
contador <- contador + 1
}
# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"
# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(datos_proc$post_content[sample(1:total_results, 1)]), con = out_file)
for (i in seq_len(nrow(datos_proc))){
# Convertimos el contenido a un objeto HTML para usar rvest
contenido_html <- rvest::read_html(datos_proc$post_content[[i]])
# Eliminamos los divs con la clase 'lee-tambien-bbcl'
contenido_html %>%
html_nodes("div.lee-tambien-bbcl") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Instagram
contenido_html %>%
html_nodes("blockquote.instagram-media") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Twitter
contenido_html %>%
html_nodes("blockquote.twitter-tweet") %>%
xml_remove()
# Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
contenido_texto <- as.character(contenido_html)
contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
# Guardamos el contenido limpio de vuelta en el data frame
datos_proc$post_content[[i]] <- contenido_texto
#Revisamos el contenido HTML resultante (opcional)
# print(as.character(contenido_html))
}
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name
)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
for (i in seq_len(nrow(datos_proc))){
# Convertimos el contenido a un objeto HTML para usar rvest
contenido_html <- rvest::read_html(datos_proc$post_content[[i]])
# Eliminamos los divs con la clase 'lee-tambien-bbcl'
contenido_html %>%
html_nodes("div.lee-tambien-bbcl") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Instagram
contenido_html %>%
html_nodes("blockquote.instagram-media") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Twitter
contenido_html %>%
html_nodes("blockquote.twitter-tweet") %>%
xml_remove()
# Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
contenido_texto <- as.character(contenido_html)
contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
# Guardamos el contenido limpio de vuelta en el data frame
datos_proc$post_content[[i]] <- contenido_texto
#Revisamos el contenido HTML resultante (opcional)
# print(as.character(contenido_html))
}
datos_proc <- datos_proc %>%
filter(grepl("inteligencia artificial", post_content, ignore.case = TRUE))
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"
# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(datos_proc$post_content[sample(1:total_results, 1)]), con = out_file)
# Mensaje de confirmación
cat("El contenido en bruto se ha exportado a", out_file)
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html
# Procesar el HTML y extraer el texto
while (contador <= total_results) {
# Convertir a nodo HTML
html_content[[contador]] <- read_html(datos_proc$post_content[[contador]])
# Extraemos y limpiamos el texto
datos_proc$post_content[[contador]] <- html_content[[contador]] %>%
html_text2() %>%
str_squish()
contador <- contador + 1
}
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
unnest_tokens(word, post_content)
View(words)
row_number(words)
rowsum(words)
nrow(words)
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2
)
rm(list = ls())
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name
)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
for (i in seq_len(nrow(datos_proc))){
# Convertimos el contenido a un objeto HTML para usar rvest
contenido_html <- rvest::read_html(datos_proc$post_content[[i]])
# Eliminamos los divs con la clase 'lee-tambien-bbcl'
contenido_html %>%
html_nodes("div.lee-tambien-bbcl") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Instagram
contenido_html %>%
html_nodes("blockquote.instagram-media") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Twitter
contenido_html %>%
html_nodes("blockquote.twitter-tweet") %>%
xml_remove()
# Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
contenido_texto <- as.character(contenido_html)
contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
# Guardamos el contenido limpio de vuelta en el data frame
datos_proc$post_content[[i]] <- contenido_texto
#Revisamos el contenido HTML resultante (opcional)
# print(as.character(contenido_html))
}
datos_proc <- datos_proc %>%
filter(grepl("inteligencia artificial", post_content, ignore.case = TRUE))
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"
# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(datos_proc$post_content[sample(1:total_results, 1)]), con = out_file)
# Mensaje de confirmación
cat("El contenido en bruto se ha exportado a", out_file)
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html
# Procesar el HTML y extraer el texto
while (contador <= total_results) {
# Convertir a nodo HTML
html_content[[contador]] <- read_html(datos_proc$post_content[[contador]])
# Extraemos y limpiamos el texto
datos_proc$post_content[[contador]] <- html_content[[contador]] %>%
html_text2() %>%
str_squish()
contador <- contador + 1
}
# Esto nos sirve sobre todo para saber si se nos coló algún bloque de código
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
unnest_tokens(word, post_content)
# Cantidad de palabras extraídas
nrow(words)
# Cargar palabras comunes en español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto"))
# Filtramos las stop words del texto
words_clean <- words %>%
anti_join(stop_words, by = "word") %>%
anti_join(stop_words_es, by = "word")
# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
count(word, sort = TRUE)
# Ver las 10 palabras más frecuentes
head(word_counts, 10)
# Graficamos las palabras más frecuentes
word_counts %>%
filter(n > 10) %>%
slice_max(n, n = 15) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col() +
coord_flip() +
labs(title = "Frecuencia de Palabras en Artículos",
x = "Palabra", y = "Frecuencia") +
theme_minimal()
View(words)
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 300  # Numero de búsquedas total al ingresar la palabra clave en emol.cl
procesados <- 0
all_data_emol <- list() # Creamos lista vacía para almacenar todo el contenido (por el formato del JSON no se puede utilizar un data frame)
combine_lists <- function(...) {
combined_list <- c(...)
return(combined_list)
} #Creamos función para combinar listas
## Iteramos hasta que el from sea menor al total de resultados
while (from < total_results) {
# Construimos el link para cada iteración (lo más importante es que el from vaya aumentando)
url <- paste0(
"https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
)
from <- from + 10 # Se aumenta el from para cada iteración (después de que se construya el link)
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
if (is.null(response)) next
data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8
json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
data2 <- list(c(json_data1[["hits"]][["hits"]][["_source"]][["texto"]]))
all_data_emol <- combine_lists(all_data_emol, data2) # Fucionamos todas las listas
procesados <- procesados +10 # Contador
print(procesados)
}
textos_df <- tibble(texto = unlist(all_data_emol)) #Convertimos la lista en data frame
#Cargamos libreria
library(pacman)
p_load(
tidyverse,
httr,
jsonlite,
dplyr,
tidytext,
ggplot2,
rvest,
stringr,
xml2
)
rm(list = ls())
## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API
# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 2060
all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
## Iteramos hasta que el offset sea menor al total de resultados
while (offset < total_results) {
# Construimos el link para cada iteración
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Se aumenta el offset para cada iteración (después de construido el link)
offset <- offset + 20
# Realizamos la solicitud y manejamos posibles errores
response <- tryCatch(
{ GET(url) },
error = function(e) {
message("Error en la conexión: ", e)
return(NULL)
}
)
# Verificamos si `response` es nulo antes de continuar
if (is.null(response)) next
# Procesamos el contenido si `response` no es nulo
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Verificamos que el elemento `notas` existe antes de unir datos
if (!is.null(json_data$notas)) {
json_notas <- json_data$notas %>%
# Convertimos a data frame y normalizar tipos
as.data.frame(stringsAsFactors = FALSE) %>%
# Convertir columnas enteras a character, si es necesario
mutate(across(where(is.integer), as.character))
# Unimos los datos al data frame previamente realizado
all_data <- bind_rows(all_data, json_notas)
}
cat(" - Procesados - ", offset) #Mostrar progreso
}
sum(is.na(all_data$post_title))
print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
datos_proc <- all_data %>%
select(
post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name
)
# Después de ejecutar el bucle y extraer todos los datos
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
for (i in seq_len(nrow(datos_proc))){
# Convertimos el contenido a un objeto HTML para usar rvest
contenido_html <- rvest::read_html(datos_proc$post_content[[i]])
# Eliminamos los divs con la clase 'lee-tambien-bbcl'
contenido_html %>%
html_nodes("div.lee-tambien-bbcl") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Instagram
contenido_html %>%
html_nodes("blockquote.instagram-media") %>%
xml_remove()
# Eliminamos los divs de publicaciones de Twitter
contenido_html %>%
html_nodes("blockquote.twitter-tweet") %>%
xml_remove()
# Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
contenido_texto <- as.character(contenido_html)
contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
# Guardamos el contenido limpio de vuelta en el data frame
datos_proc$post_content[[i]] <- contenido_texto
#Revisamos el contenido HTML resultante (opcional)
# print(as.character(contenido_html))
}
datos_proc <- datos_proc %>%
filter(grepl("inteligencia artificial", post_content, ignore.case = TRUE))
# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(datos_proc)
# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")
# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"
# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(datos_proc$post_content[sample(1:total_results, 1)]), con = out_file)
# Mensaje de confirmación
cat("El contenido en bruto se ha exportado a", out_file)
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html
# Procesar el HTML y extraer el texto
while (contador <= total_results) {
# Convertir a nodo HTML
html_content[[contador]] <- read_html(datos_proc$post_content[[contador]])
# Extraemos y limpiamos el texto
datos_proc$post_content[[contador]] <- html_content[[contador]] %>%
html_text2() %>%
str_squish()
contador <- contador + 1
}
# Esto nos sirve sobre todo para saber si se nos coló algún bloque de código
# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)
# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
unnest_tokens(word, post_content)
# Cantidad de palabras extraídas
nrow(words)
# Cargar palabras comunes en español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto"))
# Filtramos las stop words del texto
words_clean <- words %>%
anti_join(stop_words, by = "word") %>%
anti_join(stop_words_es, by = "word")
# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
count(word, sort = TRUE)
# Ver las 10 palabras más frecuentes
head(word_counts, 10)
# Graficamos las palabras más frecuentes
word_counts %>%
filter(n > 10) %>%
slice_max(n, n = 15) %>%
ggplot(aes(x = reorder(word, n), y = n)) +
geom_col() +
coord_flip() +
labs(title = "Frecuencia de Palabras en Artículos",
x = "Palabra", y = "Frecuencia") +
theme_minimal()
# Agrupamos los datos por año y mes, y contar las publicaciones
publicaciones_por_mes <- datos_proc %>%
group_by(year, month) %>%
summarise(cantidad = n(), .groups = 'drop') %>%
mutate(fecha = as.Date(paste(year, month, "01", sep = "-"))) # Crear una fecha para el eje x
# Creamos el gráfico
ggplot(publicaciones_por_mes, aes(x = fecha, y = cantidad)) +
geom_line(color = "blue", linewidth = 1) + # Línea de publicaciones
geom_point(color = "red", size = 1) + # Puntos en cada mes
geom_smooth(method = "loess", color = "green", se = FALSE, linewidth = 1) + # Curva de tendencia
labs(title = "Cantidad de Notas sobre Inteligencia Artificial Publicadas por Mes",
x = "Año",
y = "Cantidad de Notas") +
theme_minimal() +
scale_x_date(date_labels = "%Y", date_breaks = "1 year") # Mostrar solo los años en el eje x
# Contamos las publicaciones por categoría
publicaciones_por_categoria <- datos_proc %>%
group_by(post_category_primary.name) %>%
summarise(cantidad = n(), .groups = 'drop') %>%
arrange(desc(cantidad))
# Filtramos para quedarnos con las 15 categorías más usadas
top_15_categorias <- publicaciones_por_categoria %>%
top_n(15, cantidad)
# Gráfico de Barras para las 15 categorías más usadas
ggplot(top_15_categorias, aes(x = reorder(post_category_primary.name, -cantidad), y = cantidad, fill = post_category_primary.name)) +
geom_bar(stat = "identity") +
labs(title = "Top 15 Categorías más Usadas en Notas",
x = "Categoría",
y = "Cantidad de Notas") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "none")
