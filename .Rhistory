)
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=", limit,
"&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
page_links <- sapply(json_data$notas, function(nota) nota$post_URL)
all_links <- c(all_links, page_links)
# Muestra el progreso
cat("Procesados", length(all_links), "de", total_results, "resultados\n")
# Aumenta el offset para la próxima página
offset <- offset + limit
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
# Muestra o guarda todas las URLs
print(all_links)
View(response)
View(json_data)
View(response)
View(json_data[["notas"]])
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
page_links <- sapply(json_data$notas, function(nota) nota$post_URL)
all_links <- c(all_links, page_links)
# Muestra el progreso
cat("Procesados", length(all_links), "de", total_results, "resultados\n")
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
View(json_data)
View(json_data[["notas"]])
rm(list = ls())
# Configura los parámetros básicos
search_query <- "inteligencia artificial"
limit <- 10  # Número de artículos por página
total_results <- 2132  # Total inicial de artículos de la búsqueda (de acuerdo con el ejemplo JSON)
offset <- 0  # Inicia desde el primer artículo
all_links <- c()  # Almacena todas las URLs
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=", limit,
"&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas)) {
page_links <- sapply(json_data$notas, function(nota) nota$post_URL)
all_links <- c(all_links, page_links)
# Actualiza el total de resultados si ha cambiado (opcional, por si la API actualiza resultados)
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
}
# Muestra el progreso
cat("Procesados", length(all_links), "de", total_results, "resultados\n")
} else {
cat("No se encontraron notas en la respuesta de esta página.\n")
break
}
# Aumenta el offset para la próxima página
offset <- offset + limit
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas)) {
page_links <- sapply(json_data$notas, function(nota) nota$post_URL)
all_links <- c(all_links, page_links)
# Actualiza el total de resultados si ha cambiado (opcional, por si la API actualiza resultados)
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
}
# Muestra el progreso
cat("Procesados", length(all_links), "de", total_results, "resultados\n")
} else {
cat("No se encontraron notas en la respuesta de esta página.\n")
break
}
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
json_data[["notas"]]
# Configura los parámetros básicos
search_query <- "inteligencia artificial"
offset <- 0  # Inicia desde el primer artículo
all_links <- c()  # Almacena todas las URLs
rm(list = ls())
# Configura los parámetros básicos
search_query <- "inteligencia artificial"
offset <- 0  # Inicia desde el primer artículo
all_links <- c()  # Almacena todas las URLs
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
# Loop para extraer todas las páginas de resultados
while (TRUE) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas) && length(json_data$notas) > 0) {
page_links <- sapply(json_data$notas, function(nota) nota$post_URL)
all_links <- c(all_links, page_links)
# Muestra el progreso
cat("Procesados", length(all_links), "resultados\n")
} else {
cat("No se encontraron más notas. Fin de la extracción.\n")
break
}
# Actualiza el total de resultados, si está disponible
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
if (offset >= total_results) break  # Sale si llegamos al final de los resultados
}
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
View(json_data)
# Loop para extraer todas las páginas de resultados
while (TRUE) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas) && length(json_data$notas) > 0) {
page_links <- sapply(json_data$notas, function(nota) nota[[post_URL]])
all_links <- c(all_links, page_links)
# Muestra el progreso
cat("Procesados", length(all_links), "resultados\n")
} else {
cat("No se encontraron más notas. Fin de la extracción.\n")
break
}
# Actualiza el total de resultados, si está disponible
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
if (offset >= total_results) break  # Sale si llegamos al final de los resultados
}
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
View(json_data)
print(json_data$notas)
print(json_data$notas[["post_URL"]])
json_data$notas %>%
print("post_URL") %>%
print("ID")
json_data$notas %>%
print("post_URL") %>%
print("ID")
json_data$notas %>%
urls <- print("post_URL") %>%
ids <- print("ID")
json_data$notas %>%
urls <- print("post_URL") %>%
```
json_data$notas %>%
print("post_URL") %>%
```
rm(list = ls())
library(httr)
library(jsonlite)
# Configura los parámetros básicos
search_query <- "inteligencia artificial"
offset <- 0  # Inicia desde el primer artículo
all_links <- c()  # Almacena todas las URLs
total_results <- Inf  # Se inicializa con un valor alto para empezar el bucle
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas) && length(json_data$notas) > 0) {
page_links <- sapply(json_data$notas, function(nota) nota[["post_URL"]])  # `post_URL` en comillas dobles
all_links <- c(all_links, page_links)
# Muestra el progreso
cat("Procesados", length(all_links), "resultados de", total_results, "\n")
} else {
cat("No se encontraron más notas. Fin de la extracción.\n")
break
}
# Actualiza el total de resultados si es necesario
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
}
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
k
View(json_data)
# Configura los parámetros básicos
search_query <- "inteligencia artificial"
all_links <- c()  # Almacena todas las URLs
total_results <- Inf  # Se inicializa con un valor alto para empezar el bucle
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?limit=&offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas) && length(json_data$notas) > 0) {
# Filtra solo aquellos elementos que tienen `post_URL`
page_links <- sapply(json_data$notas, function(nota) {
if (!is.null(nota$post_URL)) {
return(nota$post_URL)
} else {
return(NA)  # Asigna NA si no tiene `post_URL`
}
})
# Agrega los links válidos
all_links <- c(all_links, na.omit(page_links))
# Muestra el progreso
cat("Procesados", length(all_links), "resultados\n")
} else {
cat("No se encontraron más notas. Fin de la extracción.\n")
break
}
# Actualiza el total de resultados si es necesario
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
}
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
break
}
}
ok
View(json_data)
View(response)
rm(list = ls())
library(httr)
library(jsonlite)
# Configura los parámetros básicos
search_query <- "inteligencia artificial"
offset <- 0  # Inicia desde el primer artículo
all_links <- c()  # Almacena todas las URLs
total_results <- Inf  # Se inicializa con un valor alto para empezar el bucle
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url, add_headers(.headers = headers))
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas) && length(json_data$notas) > 0) {
# Filtra solo aquellos elementos que tienen `post_URL`
page_links <- sapply(json_data$notas, function(nota) {
if (!is.null(nota[["post_URL"]])) {
return(nota[["post_URL"]])
} else {
return(NA)  # Asigna NA si no tiene `post_URL`
}
})
# Agrega los links válidos y elimina NAs
all_links <- c(all_links, na.omit(page_links))
# Muestra el progreso
cat("Procesados", length(all_links), "resultados\n")
} else {
cat("No se encontraron más notas en esta página.\n")
}
# Actualiza el total de resultados si es necesario
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
}
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
}
}
View(json_data)
View(json_data[["notas"]])
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)   offset <- offset + 20
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query)
)   offset <- offset + 20
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query)
)
offset <- offset + 20
response <- GET(url)
if (!is.null(json_data$notas) && length(json_data$notas) > 0) {
# Filtra solo aquellos elementos que tienen `post_URL`
page_links <- sapply(json_data$notas)
}
}
rm(list = ls())
# Configura los parámetros básicos
search_query <- "inteligencia artificial"
offset <- 0  # Inicia desde el primer artículo
total_results <- Inf  # Se inicializa con un valor alto para empezar el bucle
# Encabezados para la solicitud
headers <- c(
`User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
`Accept` = "application/json, text/plain, */*",
`Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
`Content-Type` = "application/json; charset=UTF-8"
)
# Loop para extraer todas las páginas de resultados
while (offset < total_results) {
# Construye la URL de la API con los parámetros actuales
url <- paste0(
"https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
"&search=", URLencode(search_query),
"&intervalo=&orden=ultimas"
)
# Realiza la solicitud a la API
response <- GET(url)
# Verifica que la solicitud sea exitosa
if (status_code(response) == 200) {
# Procesa el JSON y extrae los enlaces
data <- content(response, "text", encoding = "UTF-8")
json_data <- fromJSON(data, flatten = TRUE)
# Extrae las URLs de cada artículo y las añade a la lista de enlaces
if (!is.null(json_data$notas) && length(json_data$notas) > 0) {
# Filtra solo aquellos elementos que tienen `post_URL`
page_links <- sapply(json_data$notas, function(nota) {
if (!is.null(nota[["post_URL"]])) {
return(nota[["post_URL"]])
} else {
return(NA)  # Asigna NA si no tiene `post_URL`
}
})
# Agrega los links válidos y elimina NAs
all_links <- c(all_links, na.omit(page_links))
# Muestra el progreso
cat("Procesados", length(all_links), "resultados\n")
} else {
cat("No se encontraron más notas en esta página.\n")
}
# Actualiza el total de resultados si es necesario
if (!is.null(json_data$total)) {
total_results <- as.numeric(json_data$total)
}
# Aumenta el offset para la próxima página
offset <- offset + 20
} else {
cat("Error en la solicitud: código de estado", status_code(response), "\n")
}
}
View(json_data)
