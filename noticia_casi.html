<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html><body>
<p>Se puede considerar como <em>deepfake </em><strong>toda imagen, video y audio que imita la voz o apariencia de una persona a través de Inteligencia Artificial (IA)</strong>. </p>
<p>Sí. Aquellos covers “cantados” por personajes que no son sus autores, los videos de tu ídolo favorito deseándote feliz cumpleaños o diciendo cualquier frase que le solicites y <strong>muchas de las supuestas fotos filtradas de famosas desnudas son ejemplos de este concepto</strong>.</p>
<p>Los peligros del uso (y abuso) de estas prácticas son muchos. <strong>¿Cómo saber si lo que estoy viendo es real o no?</strong> A veces, la IA puede ser tan certera que logra confundir a la población sobre el límite entre lo verdadero y lo falso. <strong>¿Cuándo esta herramienta resulta útil y cuándo es un riesgo?</strong></p>

<p>Toda pregunta respecto al tema es válida, más aún considerando los diversos problemas y debates que ha traído el ocupar la Inteligencia Artificial para actos como lo es <strong>trucar imágenes para simular desnudos o contenido pornográfico de mujeres (o peor aún, de menores de edad) sin su consentimiento</strong>.</p>
<figure id="attachment_5806184" aria-describedby="caption-attachment-5806184" style="width: 1200px" class="wp-caption alignnone"><img decoding="async" loading="lazy" src="https://media.biobiochile.cl/wp-content/uploads/2023/09/diseno-sin-titulo-16-1.png" alt="Deepfake: Tu cara en otro cuerpo. Los montajes con IA usados para atormentar a niñas y mujeres" width="1200" height="633" class="size-full wp-image-5806184"><figcaption id="caption-attachment-5806184" class="wp-caption-text">IBO</figcaption></figure><h2>
<em>Deepfakes </em>con rostros conocidos</h2>
<p>Estas prácticas injuriosas han afectado a varias famosas alrededor del mundo. Mujeres reconocidas en diferentes rubros han sido, lamentablemente, víctimas de este tipo de montajes que atentan contra su dignidad. </p>
<p>La actriz británica <strong>Emma Watson</strong> es una de ellas, así como también la influencer y deportista madrileña <strong>Marina Rivers</strong>, la modelo española <strong>Laura Escanes</strong>, la cantante <strong>Taylor Swift</strong><strong> y la célebre <strong>Rosalía</strong>.</strong></p>
<figure id="attachment_5806189" aria-describedby="caption-attachment-5806189" style="width: 1200px" class="wp-caption alignnone"><img decoding="async" loading="lazy" src="https://media.biobiochile.cl/wp-content/uploads/2023/09/diseno-sin-titulo-17-1.png" alt="Deepfake: Tu cara en otro cuerpo. Los montajes con IA usados para atormentar a niñas y mujeres" width="1200" height="633" class="size-full wp-image-5806189"><figcaption id="caption-attachment-5806189" class="wp-caption-text">El Universo</figcaption></figure><p>Si bien las reacciones de las chicas mencionadas tuvieron tonos diferentes entre sí, hubo una aseveración en común en ciertas de sus declaraciones frente al tema: <strong>el cuerpo de una mujer no se utiliza ni es de propiedad pública</strong>.</p>

<p> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>

<p> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
<h2>Nadie está libre: Los casos de <em>Deepfake </em>en Almendralejo y Lima</h2>
<h2>
</h2>
<p>Hace tan sólo unos días en España, más específicamente en la ciudad de Almendralejo, un caso ligado a esta práctica causó conmoción. <strong>Más de una quincena de niñas entre los 11 y 17 años denunciaron haber sido víctima de <em>deepfakes </em>por parte de compañeros de curso</strong>, quienes difundieron el material pornográfico falso creado por IA con la cara de las menores a través de Whatsapp.</p>
<p>Este incidente no sólo desconcerta por la intencionalidad y poco criterio que significa, sino porque también se ha transformado en <strong>el primer caso masivo de <em>deepfake </em>en España</strong>.</p>
<figure id="attachment_5806190" aria-describedby="caption-attachment-5806190" style="width: 1200px" class="wp-caption alignnone"><img decoding="async" loading="lazy" src="https://media.biobiochile.cl/wp-content/uploads/2023/09/diseno-sin-titulo-18-1.png" alt="Deepfake: Tu cara en otro cuerpo. Los montajes con IA usados para atormentar a niñas y mujeres" width="1200" height="633" class="size-full wp-image-5806190"><figcaption id="caption-attachment-5806190" class="wp-caption-text">Pexels</figcaption></figure><p>Hasta ahora, la policía nacional de dicho país ha identificado a <strong>11 menores como los responsables del delito, quienes por tener menos de 14 años resultan en individuos inimputables</strong> bajo la ley hispana. De no ser porque los niños tienen entre 12 y 13 años, éstos podrían arriesgar hasta dos años de cárcel como castigo por parte del Código Penal de su nación.</p>
<p>Un suceso similar ocurrió hace casi un mes en Lima, Perú. Hay varias similitudes entre los dos casos: <strong>ambos ocurrieron en colegios, las víctimas fueron niñas y los culpables niños… Todos los involucrados son menores de edad</strong>. </p>
<p>Dos alumnos del establecimiento educacional peruano trucaron fotografías de 12 compañeras con inteligencia artificial, transformándolas en <strong>imágenes sexuales que terminaron vendiendo por Internet</strong>.</p>

<p>Este tipo de delitos trae a colación una reflexión: <strong>la importancia de la educación sobre el uso responsable de la tecnología</strong>. </p>
<p>Tan sólo este año, la Oficina Federal de Investigación de los Estados Unidos (FBI) advirtió sobre el considerable aumento de <strong><em>deepfakes </em>para la “sextorsión”</strong>: amenazas y chantaje sobre la difusión del contenido creado por Inteligencia Artificial para conseguir dinero u otro tipo de acciones a cambio.</p>
<p>En un comunicado público, el FBI expuso lo siguiente: </p>
<blockquote><p>“A partir de abril de 2023, el FBI ha observado un incremento de las víctimas de extorsión sexual que informan del uso de imágenes o videos falsos creados a partir de contenido publicado en sus redes sociales o publicaciones web, proporcionados al actor malicioso mediante solicitud del mismo, o que fue tomado mediante capturas de pantalla durante chats de video. De acuerdo con los informes recientes de las víctimas, los actores maliciosos suelen exigir: 1. Un pago (por ejemplo, dinero, tarjetas regalo) con la amenaza de compartir las imágenes o videos con miembros de la familia o amigos (de la víctima) en las redes sociales si no se recibían los fondos; o 2. El envío por parte de la víctima de imágenes o videos reales de temática sexual”</p></blockquote>
<p>De tal manera, queda en evidencia la lamentable masificación de este tipo de prácticas alrededor del mundo.</p>
<h2>Acciones contra el <em>deepfake</em>
</h2>
<p>En junio de este año, el Parlamento Europeo aprobó con 499 votos a favor, 28 en contra y 93 abstenciones <strong>un proyecto de ley que regula el uso de la Inteligencia Artificial</strong>. El texto que se ejecutará deberá ser pactado entre los 27 Estados miembros de la Unión Europea (UE). Se estima que dicho plan esté aprobado para fines de este año.</p>
<p>Sin embargo, este futuro proyecto <strong>no se refiere a casos puntuales de <em>deepfake</em></strong> como fueron los que perjudicaron a las niñas de Almendralejo y Perú. La Unión Europea tendría que ajustar su ley de IA para adaptarla a los casos de desnudos <em>fake</em>. </p>
<p>Si aquel fuera el caso, estaríamos hablando de una <strong>medida pionera internacionalmente hablando</strong>; una determinación que velaría por la sanción de los culpables detrás de los <em>deepfakes</em>, además de la protección de las víctimas de los mismos que para variar son, en su mayoría, mujeres y niñas de cualquier latitud.</p>
<figure id="attachment_5806553" aria-describedby="caption-attachment-5806553" style="width: 1200px" class="wp-caption alignnone"><img decoding="async" loading="lazy" src="https://media.biobiochile.cl/wp-content/uploads/2023/09/diseno-sin-titulo-22-1.png" alt="Deepfake: Tu cara en otro cuerpo. Los montajes con IA usados para atormentar a mujeres y niñas" width="1200" height="633" class="size-full wp-image-5806553"><figcaption id="caption-attachment-5806553" class="wp-caption-text">Pixabay</figcaption></figure>
</body></html>

