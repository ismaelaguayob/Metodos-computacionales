---
title: "Tarea 1: scrapping"
format: html
editor: visual
eval: false
echo: true
---

## Web-Scrapping

El siguiente documento dinámico presentará el proceso de web-scapping que se realizó para el ramo "Métodos Computacionales para las Ciencias Sociales".

```{r}

#Cargamos libreria

library(pacman)

p_load(
  tidyverse,
  httr,
  jsonlite,
  dplyr,
  tidytext,
  ggplot2,
  rvest,
  stringr,
  xml2
)

rm(list = ls()) 
setwd("~/GitHub/Metodos-computacionales")
```

## Obtenemos el texto de las noticias con la búsqueda "Inteligencia Artificial" en medio Bío Bío

```{r}

## Parámetros básicos
search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API

# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (hay datos que al parecer están codificados de otra forma, así que pusimos un número menor al total, que toma notas de prensa desde el año 2016 en adelante, tiene que ser múltiplo de 20)
total_results <- 2020  

all_data <- data.frame() # Creamos data.frame vacío para luego extraer los datos de la API

# Encabezados para la solicitud
headers <- c(
  `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
  `Accept` = "application/json, text/plain, */*",
  `Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
  `Content-Type` = "application/json; charset=UTF-8"
)

## Iteramos hasta que el offset sea menor al total de resultados

while (offset < total_results) {
  # Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
  url <- paste0(
    "https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
    "&search=", URLencode(search_query),
    "&intervalo=&orden=ultimas"
  )
    offset <- offset + 20 # Se aumenta el offset para cada iteración (después de que se construya el link)
  response <- GET(url) # Realizamos la solicitud
    data <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8 
    json_data <- fromJSON(data, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
    json_notas <- c(json_data$notas) # Creamos un data frame donde solo esté la información relevante
    all_data <- bind_rows(all_data, json_notas)
    #cat(" - Procesados - ", offset) #Mostrar progreso, se puede desactivar con #
}

# print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página
```

#### Sacamos un data frame con las columnas que nos interesen

```{r}

datos_proc <- all_data %>% 
  select(
    post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name
    )
```

## Limpieza de las secciones "Lee también..."

Aparecen algunos bloques html que ponen noticias como recomedación para leer y pueden contener las palabras "inteligencia artificial" sin que el contenido de la nota se refiera específicamente a eso. Para ello podemos usar el paquete rvest

#### Necesitamos una función que se dedique a encontrar esa parte del texto y eliminarla:

```{r}

for (i in seq_len(nrow(datos_proc))){
  # Convertimos el contenido a un objeto HTML para usar rvest
  contenido_html <- rvest::read_html(datos_proc$post_content[[i]])

  # Eliminar los divs con la clase 'lee-tambien-bbcl'
  contenido_html %>%
    html_nodes("div.lee-tambien-bbcl") %>%
    xml_remove()

  # Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
  contenido_texto <- as.character(contenido_html)
  contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
  
  # Guardamos el contenido limpio de vuelta en el data frame
  datos_proc$post_content[[i]] <- contenido_texto

  #Revisamos el contenido HTML resultante (opcional)
  # print(as.character(contenido_html))
}
```

#### También podemos exportar como html para ver si desaparecieron los bloques completos

```{r}

# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"

# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(datos_proc$post_content[sample(1:total_results, 1)]), con = out_file)

# Mensaje de confirmación 
cat("El contenido en bruto se ha exportado a", out_file) 
```

## Limpieza de texto general

Eliminamos la parte de código que queda en el texto para convertirlo en texto plano.

```{r}
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html

# Procesar el HTML y extraer el texto
while (contador <= total_results) {
  # Convertir a nodo HTML
  html_content[[contador]] <- read_html(datos_proc$post_content[[contador]])
  
  # Extraemos y limpiamos el texto
  datos_proc$post_content[[contador]] <- html_content[[contador]] %>%
    html_text2() %>% 
    str_squish()
  
  contador <- contador + 1
}
```

## Descriptivos de frecuencia de palabras

```{r}

##Bio Bio

# Seleccionamos solo la columna de texto que nos interesa
text_data <- datos_proc %>% select(post_content)

# Tokenizamos el texto y lo dividimos en palabras
words <- datos_proc %>%
  unnest_tokens(word, post_content)

# Cargar palabras comunes en español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto"))

# Filtrar las stop words del texto
words_clean <- words %>%
  anti_join(stop_words, by = "word") %>%
  anti_join(stop_words_es, by = "word")

# Calcular frecuencia de palabras
word_counts <- words_clean %>%
  count(word, sort = TRUE)

# Ver las 10 palabras más frecuentes
head(word_counts, 10)

# Graficar las palabras más frecuentes
word_counts %>%
  filter(n > 10) %>%
  top_n(15, n) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Frecuencia de Palabras en Artículos",
       x = "Palabra", y = "Frecuencia") +
  theme_minimal()
```
