---
title: "Tarea 1: scrapping"
format: html
editor: visual
echo: true
css: styles.css 
---

# Por Hacer:

-   Explorar los modelos de pregunta, con los párrafos generados (Google colab)

-   Arreglar la entrega de Opazo, unidad de análisis Technological Frame

# Primer trabajo métodos computacionales

#### *Integrantes: Ismael Aguayo y Exequiel Trujillo*

## Relevancia de los datos

El uso de técnicas de extracción de datos por medio de APIs permite obtener grandes volúmenes de información de fuentes digitales, lo cual es particularmente valioso para las ciencias sociales, que tradicionalmente se han basado en encuestas y censos que pueden ser más costosos y limitados en tiempo y alcance.

Las técnicas de web scraping permiten explorar temas actuales y en constante cambio (mientras se publican contenidos nuevos a diario). Quienes hacen investigación en ciencias sociales necesitan adaptarse a entornos digitales dinámicos. A través de este proyecto se espera aprender a integrar y desarrollar herramientas computacionales en la recolección y análisis de datos.

La relevancia sociológica de los datos que se utilizarán se sostiene, en principio, en el elemento revolucionario y transformador que presenta una tecnología como la IA. Segundo, la utilización de medios masivos de comunicación como BíoBíocl y Emol, ambos los medios online más visitados en el país. Esto vinculado con el gran volumen de datos que se está utilizando, otorga validez y significancia a los análisis sociales que se van a realizar. Como se demostró en la introducción, la IA ha permeado el trabajo académico en muchos ámbitos, siendo de los más visitados entre estos desde la sociología, la educación y el trabajo. Sin embargo, se percibe una vacío académico en análisis constructivistas que perciban la tecnología no solo como un objeto técnico, sino uno imbricado socialmente con significados, prácticas, y con un gran impacto en la cultura; uno construido en un proceso simultáneamente técnico como social. En los medios, esta construcción es evidente, y tiene un impacto mayor en los significados construidos por los individuos tanto por su masividad como por su posición privilegiada en la sociedad como emisor de información con validez. 

## Contexto BíoBío y Emol

Para este proyecto, decidimos trabajar con Emol y Bío Bío, dos de los medios de noticias más conocidos en Chile y que tienen un gran impacto en cómo la gente entiende lo que pasa en el país. Nos interesa especialmente porque ambos cubren temas de política, economía, sociedad, tecnología y cultura, en particular sobre Inteligencia Artificial, lo que nos ayuda a explorar cómo se presentan y enmarcan temas importantes o de impacto como este.

Emol es parte del grupo El Mercurio, uno de los conglomerados de medios más grandes de Chile. Este medio tiene una fuerte influencia en la opinión pública y su contenido se enfoca mucho en temas económicos, políticos y empresariales. Creemos que analizar artículos de Emol nos permite ver cómo se representan ciertos sectores de la sociedad y observar el tipo de lenguaje que usan en temas sensibles o de alto interés para la gente. El medio de comunicación El Mercurio On-Line (Emol) es parte de la empresa El Mercurio S.A.P., siendo su portal digital de noticias. Nació a mediados de los 90s con el objetivo de informar a las empresas en economía y actualidad, sin embargo, debido a la competencia de La Tercera online y Terra, se convirtió en lo que es hoy, publicando el contenido de El Mercurio, La segunda y Las Últimas Noticias (Wikipedia, 2024).

Por otro lado, Bío Bío tiene un enfoque más amplio y diverso, pero sigue siendo uno de los medios más consumidos en Chile. El medio BíoBío.cl, del que se extraerán los datos del presente estudio, data de 1959. En principio era una radio, siendo la única con sede fuera de la capital. Para el año 2009 se creó la página web que se utilizará, la cual en 2020 es la página web de noticias más visitada del país, con 122 millones de usuarios únicos y 595 millones de visitas (Quiénes somos \| BioBiochile, s. f.).

Ambos medios son los más visitados digitalmente en el país, estando en las posiciones primera (Emol) y segunda (BioBiocl), según el sitio web de rankings SimilarWeb (Septiembre de 2024). 

```{r}

#Cargamos libreria

library(pacman)

p_load(
  tidyverse,
  httr,
  jsonlite,
  dplyr,
  tidytext,
  ggplot2,
  ggvenn,
  rvest,
  stringr,
  xml2,
  wordcloud2,
  arrow,
  readxl,
  stopwords,
  lubridate,
)

rm(list = ls()) 
```

## Obtenemos el texto de las noticias con la búsqueda "Inteligencia Artificial" en medio Bío Bío

Se utilizó la API de búsqueda que utiliza la web de BíoBío.cl. Esta no cuenta con documentación, por lo que a prueba y error se fueron descubriendo los parámetros necesarios para automatizar el scrappeo del contenido de la web. Los headers que se utilizaron son los mismos que arroja el get cuando uno realiza una busqueda desde el navegador.

```{r}

## Parámetros básicos

search_query <- "Inteligencia Artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API

# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 0

all_data <- data.frame(ID = character(),  # Definimos columnas vacías
                       post_content = character(),
                       post_title = character(),
                       year = integer(),  # Año como entero
                       month = integer(),  # Mes como entero
                       day = integer(),  # Día como entero
                       post_category_primary.name = character(),
                       post_category_secondary.name = character(),
                       stringsAsFactors = FALSE)  # Inicializamos dataframe vacío

# Encabezados para la solicitud
headers <- c(
  `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
  `Accept` = "application/json, text/plain, */*",
  `Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode(search_query)),
  `Content-Type` = "application/json; charset=UTF-8"
)

# URL de la solicitud inicial
url_initial <- paste0(
  "https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
  "&search=", URLencode(search_query),
  "&intervalo=&orden=ultimas"
)

# Realizar la solicitud
response_initial <- GET(url_initial, add_headers(.headers = headers))

# Procesar la respuesta
if (response_initial$status_code == 200) {
  # Convertir el contenido a texto y luego a JSON
  data_initial <- content(response_initial, "text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE)
  
  # Extraer el total de resultados
  if (!is.null(data_initial$total)) {
    total_results <- as.numeric(data_initial$total)
    message("Número total de resultados disponibles: ", total_results)
  } else {
    stop("No se encontró el parámetro 'total' en la respuesta.")
  }
} else {
  stop("Error al realizar la solicitud inicial. Código de estado: ", response_initial$status_code)
}

## Iteramos hasta que el offset sea menor al total de resultados

while (offset < total_results) {
  # Construimos el link para cada iteración
  url <- paste0(
    "https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
    "&search=", URLencode(search_query),
    "&intervalo=&orden=ultimas"
  )
  
  # Se aumenta el offset para cada iteración (después de construido el link)
  offset <- offset + 20
    
  # Realizamos la solicitud y manejamos posibles errores
  response <- tryCatch(
    { GET(url) },
    error = function(e) { 
      message("Error en la conexión: ", e) 
      return(NULL)
    }
  )
    
  # Verificamos si `response` es nulo antes de continuar
  if (is.null(response)) next
  
  # Procesamos el contenido si `response` no es nulo
  data <- content(response, "text", encoding = "UTF-8")
  json_data <- fromJSON(data, flatten = TRUE)

  # Verificamos que el elemento `notas` existe antes de unir datos
  if (!is.null(json_data$notas)) {
    json_notas <- json_data$notas %>%
      # Verificamos si las columnas existen y les asignamos valores o NA
  mutate(
    post_category_primary.name = ifelse("post_category_primary.name" %in% names(.), 
                                   json_data$notas$post_category_primary.name, NA),
    post_category_secondary.name = ifelse("post_category_secondary.name" %in% names(.), 
                                     json_data$notas$post_category_secondary.name, NA)
  ) %>%
      select(ID,  
             post_content, 
             post_title,  
             year, 
             month,  
             day, 
             post_category_primary.name,  
             post_category_secondary.name) %>%
      mutate(
        across(everything(), as.character),  # Convertimos todo a caracteres primero
        year = as.integer(year),  # Convertimos 'year' a entero
        month = as.integer(month),  # Convertimos 'month' a entero
        day = as.integer(day)  # Convertimos 'day' a entero
      )
    
    # Combinamos los nuevos datos con los datos existentes
    all_data <- bind_rows(all_data, json_notas)
  }
  
  cat(" - Procesados - ", offset, "\n") # Mostrar progreso
}

print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página

```

## Extraemos los párrafos que mencionan inteligencia artificial o IA

```{r}

# Crear vector con sinónimos en caso de que sea necesario
sinonimos_search_query <- c("IA", "AI")

# (?i): Activamos la insensibilidad a mayúsculas y minúsculas
# \\b: Aseguramos que las palabras coincidan completas, evitando falsos positivos en palabras como "parcial" que contiene "IA".
pattern <- paste0("(?i)\\b(", search_query, "|", paste(sinonimos_search_query, collapse = "|"), ")\\b")

# Expresión regular optimizada para buscar la search_query o sinónimos
all_data <- all_data %>%
  mutate(
    parrafos_filtrados = map(post_content, ~ {
      
      # (.x): Seleccionamos el elemento actual de post_content dentro de la iteración
      # Con try catch para que sea NA en caso de error y no detenga la iteración
      nodo_html <- tryCatch(read_html(.x), error = function(e) return(NA))
      
      # Verificamos que el nodo HTML fue leído correctamente.
      if (!is.na(nodo_html)) {
        parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
        parrafos[grepl(pattern, parrafos)]
      } else {
        NA # En caso de que hubo un error al leer el html
      }
    })
  )

print(all_data$parrafos_filtrados[1])
print(sum(is.na(all_data$parrafos_filtrados)))
```

## Limpieza de las secciones "Lee también...", Instagram y Twitter

Aparecen algunos bloques html que ponen noticias como recomedación para leer y pueden contener las palabras "inteligencia artificial" sin que el contenido de la nota se refiera específicamente a eso. Para ello podemos usar el paquete rvest. Lo mismo haremos con los bloques que tienen publicaciones de X (ex Twitter) o de Instagram. Así, mantendremos solo el texto de la noticia y eliminaremos las noticias que se agregaron sin que sean relevantes.

#### Necesitamos una función que se dedique a encontrar esa parte del texto y eliminarla:

```{r}

for (i in seq_len(nrow(all_data))){
  # Convertimos el contenido a un objeto HTML para usar rvest
  contenido_html <- rvest::read_html(all_data$post_content[[i]])

  # Eliminamos los divs con la clase 'lee-tambien-bbcl'
  contenido_html %>%
    html_nodes("div.lee-tambien-bbcl") %>%
    xml_remove()

  # Eliminamos los divs de publicaciones de Instagram
  contenido_html %>%
    html_nodes("blockquote.instagram-media") %>%
    xml_remove()

  # Eliminamos los divs de publicaciones de Twitter
  contenido_html %>%
    html_nodes("blockquote.twitter-tweet") %>%
    xml_remove()

  # Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
  contenido_texto <- as.character(contenido_html)
  contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
  
  # Guardamos el contenido limpio de vuelta en el data frame
  all_data$post_content[[i]] <- contenido_texto

  #Revisamos el contenido HTML resultante (opcional)
  # print(as.character(contenido_html))
}

```

#### Ahora eliminamos todas las notas que no mencionen Inteligencia Artificial por lo menos una vez:

```{r}

# Identificamos las filas que NO contienen la búsqueda o sus sinónimos
indices_no_match <- which(!grepl(pattern, all_data$post_content, ignore.case = TRUE))

# Test
# print(all_data$post_content[indices_no_match[1]])

# Eliminamos las filas que no contienen el término de búsqueda usando los índices
all_data <- all_data[-indices_no_match, ]

# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(all_data)

# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")

# Confirmación de las filas eliminadas
cat("Los índices de las filas eliminadas son:", indices_no_match, "\n")
```

#### También podemos exportar como html para ver si desaparecieron los bloques completos

```{r}

# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"

# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(all_data$post_content[sample(1:total_results, 1)]), con = out_file)

# Mensaje de confirmación 
cat("El contenido en bruto se ha exportado a", out_file) 
```

## Limpieza de texto general

Eliminamos la parte de código que queda en el texto para convertirlo en texto plano.

```{r}
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html

# Procesar el HTML y extraer el texto
while (contador <= total_results) {
  # Convertir a nodo HTML
  html_content[[contador]] <- read_html(all_data$post_content[[contador]])
  
  # Extraemos y limpiamos el texto
  all_data$post_content[[contador]] <- html_content[[contador]] %>%
    html_text2() %>% 
    str_squish()
  
  contador <- contador + 1
}
```

## Descriptivos de frecuencia de palabras

```{r}

# Seleccionamos solo la columna de texto que nos interesa
text_data <- all_data %>% select(post_content)

# Tokenizamos el texto y lo dividimos en palabras
words <- all_data %>%
  unnest_tokens(word, post_content)

# Cantidad de palabras extraídas

nrow(words)

# Cargar palabras comunes en inglés y español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es_comunes <- stopwords("es",source ="snowball")
stop_words_es_comunes <- tibble(word = stop_words_es_comunes)

# Si es necesario, crear un tibble con palabras que particularmente para un search_query específico no interesan
stop_words_particulares <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "tu", "siempre", "2", "real", "realidad", "había", "5", "12", "2023", "2021", "muchas", "va"))

# Filtramos las stop words del texto
words_clean <- words %>%
  anti_join(stop_words, by = "word") %>%
  anti_join(stop_words_es_comunes, by = "word") %>%
  anti_join(stop_words_particulares)

# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
  count(word, sort = TRUE)

# Ver las 10 palabras más frecuentes
head(word_counts, 10)

# Realizamos nube de palabras, más frecuentes

word_counts_filtered <- word_counts %>% filter(n > 200) %>% slice_max(n, n = 70)

wordcloud2(
  data = word_counts_filtered, 
  size = 0.3,              # Aumenta el tamaño general de las palabras
  minSize = 0,             # Asegura que todas las palabras sean visibles
  gridSize = 1,            # Ajusta la densidad de palabras
  color = "random-dark",   # Colores para las palabras
  backgroundColor = "white", # Fondo blanco
  shape = "circle",        # Forma circular para compactar la nube
  ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
```

## Descriptivo cantidad de notas por meses

```{r}

# Agrupamos los datos por año y mes, y contar las publicaciones
publicaciones_por_mes <- all_data %>%
  group_by(year, month) %>%
  summarise(cantidad = n(), .groups = 'drop') %>%
  mutate(fecha = as.Date(paste(year, month, "01", sep = "-"))) # Crear una fecha para el eje x

# Crear título dinámico usando search_query
titulo_grafico <- paste("Cantidad de Notas sobre", search_query, "publicadas por mes")

# Creamos el gráfico
ggplot(publicaciones_por_mes, aes(x = fecha, y = cantidad)) +
  geom_line(color = "blue", linewidth = 1) + # Línea de publicaciones
  geom_point(color = "red", size = 1) + # Puntos en cada mes
  geom_smooth(method = "loess", color = "green", se = FALSE, linewidth = 1) + #   Curva de tendencia
  labs(title = titulo_grafico, # Usamos el título dinámico
       x = "Año",
       y = "Cantidad de Notas") +
  theme_minimal() +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") # Mostrar solo los años en el eje x
```

Se visualiza un aumento exponencial de noticias que se condice con el boom de la inteligencia artificial (IA) a fines del 2022 por la salida de ChatGPT. Luego hay picos altos y bajos, sin embargo nunca se vuelve a la frecuencia antes del boom inicial. Podría ser interesante investigar las fechas de los picos altos para ver si ocurrieron hechos importantes en relación a la IA.

## Descriptivo cantidad de notas por categoría

```{r}

# Contamos las publicaciones por categoría
publicaciones_por_categoria <- all_data %>%
  group_by(post_category_primary.name) %>%
  summarise(cantidad = n(), .groups = 'drop') %>%
  arrange(desc(cantidad))

# Filtramos para quedarnos con las 15 categorías más usadas
top_15_categorias <- publicaciones_por_categoria %>%
  top_n(15, cantidad)

# Gráfico de Barras para las 15 categorías más usadas
ggplot(top_15_categorias, aes(x = reorder(post_category_primary.name, -cantidad), y = cantidad, fill = post_category_primary.name)) +
  geom_bar(stat = "identity") +
  labs(title = paste("15 categorías más usadas en notas de Bío-Bío sobre", search_query),
       x = "Categoría",
       y = "Cantidad de Notas") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none")
```

Como era de esperarse, el tema más recurrente es Ciencia y Tecnología, seguido de Economía, Nacional, Artes y Cultura y Opinión.

## Guardamos el data frame procesado en formato parquet

```{r}
arrow::write_parquet(all_data, "data.parquet")
```

## Importamos el parquet actualizado

Luego de procesar en el notebook de Google Colab, importamos los resultados con análisis de sentimiento.

```{r}
# Leer el archivo Parquet exportado
all_data_updated <- read_parquet("all_data_updated.parquet")

# Verificar los datos
head(all_data_updated)
```

## Procesamos el nuevo data frame

```{r}
# Definimos un umbral para considerar las valoraciones positivas o negativas
umbral_sentimiento <- 0.99

# Calculamos el sentimiento predominante basado en los puntajes y el umbral
all_data_updated <- all_data_updated %>%
  mutate(
    predominant_sentiment = sapply(sentiment, function(x) {
      # Filtramos y calculamos los puntajes de sentimientos positivos y negativos que superan el umbral
      positive_score <- sum(ifelse(x$label == "Positive" & x$score > umbral_sentimiento, x$score, 0))
      negative_score <- sum(ifelse(x$label == "Negative" & x$score > umbral_sentimiento, x$score, 0))

      # Determinamos el sentimiento predominante basado en la suma de los puntajes
      if (positive_score > negative_score) {
        return("Positive")
      } else if (negative_score > positive_score) {
        return("Negative")
      } else {
        return("Neutral")  # En caso de empate o cuando ninguno supera el umbral
      }
    })
  )

# Verificar el resultado
head(all_data_updated)
```

## Descriptivos con los nuevos datos

```{r}

# Filtramos las filas que son "Positive" o "Negative" (eliminando "Neutral")
filtered_data <- all_data_updated %>%
  filter(predominant_sentiment %in% c("Positive", "Negative"))

# Contamos las notas por mes y sentimiento
monthly_sentiment_count <- filtered_data %>%
  mutate(fecha = as.Date(paste(year, month, "01", sep = "-"))) %>%  # Creamos una fecha para el eje x
  group_by(fecha, predominant_sentiment) %>%
  summarise(note_count = n(), .groups = "drop")

# Crear título dinámico usando search_query
titulo_grafico <- paste("Cantidad de Notas sobre", search_query, "por Año y Sentimiento Predominante")

# Graficamos
ggplot(monthly_sentiment_count, aes(x = fecha, y = note_count, color = predominant_sentiment, group = predominant_sentiment)) +
  geom_line(linewidth = 1, alpha = 0.2) +  # Líneas con baja opacidad
  geom_smooth(method = "loess", aes(color = predominant_sentiment), se = FALSE, size = 1.5) +  # Línea de tendencia más visible
  labs(
    title = titulo_grafico,
    x = "Año",
    y = "Cantidad de Notas",
    color = "Sentimiento Predominante"
  ) +
  theme_minimal() +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +  # Mostrar solo los años en el eje x
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Mejor presentación del eje x

```

## Word Clouds por sentimiento predominante

```{r}
# Filtrar palabras para párrafos con sentimiento predominante "Positive"
positive_words <- all_data_updated %>%
  filter(predominant_sentiment == "Positive") %>%  # Filtramos solo los positivos
  unnest_tokens(word, post_content) %>%            # Tokenizamos palabras
  anti_join(stop_words, by = "word") %>%           # Quitamos palabras comunes en inglés
  anti_join(stop_words_es_comunes, by = "word") %>%# Quitamos palabras comunes en español
  anti_join(stop_words_particulares, by = "word")  # Quitamos palabras específicas irrelevantes

# Calculamos frecuencia de palabras positivas
positive_word_counts <- positive_words %>%
  count(word, sort = TRUE)

# Filtrar palabras para párrafos con sentimiento predominante "Negative"
negative_words <- all_data_updated %>%
  filter(predominant_sentiment == "Negative") %>%  # Filtramos solo los negativos
  unnest_tokens(word, post_content) %>%            # Tokenizamos palabras
  anti_join(stop_words, by = "word") %>%           # Quitamos palabras comunes en inglés
  anti_join(stop_words_es_comunes, by = "word") %>%# Quitamos palabras comunes en español
  anti_join(stop_words_particulares, by = "word")  # Quitamos palabras específicas irrelevantes

# Calculamos frecuencia de palabras negativas
negative_word_counts <- negative_words %>%
  count(word, sort = TRUE)

# Word cloud para palabras positivas
positive_wordcloud_data <- positive_word_counts %>%
  filter(n > 50) %>%               # Filtro de palabras con más de 50 apariciones (ajusta según el dataset)
  slice_max(n, n = 70)             # Tomamos las 70 palabras más frecuentes

wordcloud2(
  data = positive_wordcloud_data, 
  size = 0.3,
  minSize = 0,
  gridSize = 1,
  color = "random-light",          # Usamos colores claros para diferenciarlas
  backgroundColor = "black",       # Fondo negro
  shape = "circle",
  ellipticity = 1
)

# Word cloud para palabras negativas
negative_wordcloud_data <- negative_word_counts %>%
  filter(n > 30) %>%               # Filtro de palabras con más de 50 apariciones (ajusta según el dataset)
  slice_max(n, n = 70)             # Tomamos las 70 palabras más frecuentes

wordcloud2(
  data = negative_wordcloud_data, 
  size = 0.3,
  minSize = 0,
  gridSize = 1,
  color = "random-dark",           # Usamos colores oscuros para diferenciarlas
  backgroundColor = "white",       # Fondo blanco
  shape = "circle",
  ellipticity = 1
)
```

## Graficamos los conjuntos de palabras top

```{r}

# Crear conjuntos para las palabras más frecuentes
top_positive_words <- positive_word_counts %>%
  slice_max(n, n = 50) %>% # Tomamos las 50 palabras más frecuentes
  pull(word)

top_negative_words <- negative_word_counts %>%
  slice_max(n, n = 50) %>% # Tomamos las 50 palabras más frecuentes
  pull(word)

# Crear subconjuntos de palabras
unique_positive <- setdiff(top_positive_words, top_negative_words) # Solo positivas
unique_negative <- setdiff(top_negative_words, top_positive_words) # Solo negativas
shared_words <- intersect(top_positive_words, top_negative_words)  # Compartidas

# Crear lista para el gráfico de conjuntos
word_sets <- list(
  Positive = top_positive_words,
  Negative = top_negative_words
)

# Crear gráfico de Venn
ggvenn(
  word_sets, 
  fill_color = c("#00AFBB", "#E7B800"),  # Colores diferenciados
  stroke_size = 0.5,                    # Grosor de las líneas
  set_name_size = 5                     # Tamaño de los nombres
)

# Crear un data frame con las palabras y su categoría
word_categories <- data.frame(
  word = c(unique_positive, unique_negative, shared_words),
  category = c(
    rep("Positive Only", length(unique_positive)),
    rep("Negative Only", length(unique_negative)),
    rep("Shared", length(shared_words))
  )
)

# Palabras solo en positivas
unique_positive_df <- word_categories %>% filter(category == "Positive Only")
print("Palabras únicas de positivas:")
print(unique_positive_df)

# Palabras solo en negativas
unique_negative_df <- word_categories %>% filter(category == "Negative Only")
print("Palabras únicas de negativas:")
print(unique_negative_df)

# Palabras compartidas
shared_words_df <- word_categories %>% filter(category == "Shared")
print("Palabras compartidas:")
print(shared_words_df)

# Opcional: Guardar las palabras en un archivo CSV
# write.csv(word_categories, "word_venn_categories.csv", row.names = FALSE)
```

# Web-scrapping utilizando la API de Emol

Lo que sigue de aquí en adelante se encuentra incompleto, sin embargo, se consideró relevante incluirlo en la entrega para poder recibir una retroalimentación. El procedimiento es el mismo: se encontró una API no documentada que utiliza el buscador de Emol. Luego se fueron descubriendo a prueba y error los parámetros necesarios para iterar. La mayor complicación que se tuvo es que la estructura de datos que entrega la API difiere mucho en este caso. La API entrega las noticias agrupadas de diez en diez.

```{r}

# Parámetros básicos
search_query <- "Inteligencia Artificial"  # Palabra clave para obtener los artículos
offset <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results_emol <- 0  # Número total de resultados de la búsqueda

# Definir URL base y parámetros de la consulta
base_url <- "https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes"
size <- 10  # Número de artículos por página

# Inicializar el data frame vacío
all_data_emol <- data.frame(
  ID = character(),  # Definimos las columnas vacías
  post_content = character(),
  post_title = character(),
  year = integer(),  # Año como entero
  month = integer(),  # Mes como entero
  day = integer(),  # Día como entero
  seccion = character(),
  subSeccion = character(),
  stringsAsFactors = FALSE  # Inicializamos el data frame sin factores
)

# Obtener el número total de resultados
url_initial <- paste0(
  base_url, 
  "?q=", URLencode(search_query), 
  "&size=1&from=0"
)

response_initial <- GET(url_initial)
if (status_code(response_initial) == 200) {
  data_initial <- content(response_initial, "text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE)
  
  if (!is.null(data_initial$hits$total)) {
    total_results_emol <- as.numeric(data_initial$hits$total)
    message("Número total de resultados: ", total_results_emol)
  } else {
    stop("No se encontró el parámetro 'total' en la respuesta.")
  }
} else {
  stop("Error en la solicitud inicial. Código de estado: ", status_code(response_initial))
}

# Iterar por todas las páginas de resultados
while (offset < total_results_emol) {
  # Construir la URL para cada iteración
  url <- paste0(
    base_url, 
    "?q=", URLencode(search_query), 
    "&size=", size, 
    "&from=", offset
  )
  
  # Aumentar el offset para la siguiente iteración
  offset <- offset + size
  
  # Realizar la solicitud y manejar posibles errores
  response <- tryCatch(
    { GET(url) },
    error = function(e) { 
      message("Error en la conexión: ", e) 
      return(NULL)
    }
  )
  
  if (is.null(response)) next
  
  # Procesar el contenido si la respuesta es válida
  data <- content(response, "text", encoding = "UTF-8")
  json_data <- fromJSON(data, flatten = TRUE)
  
  # Verificar si hay datos en `hits$hits` antes de procesar
  if (!is.null(json_data$hits$hits)) {
    noticias <- json_data$hits$hits  # Esto es un data.frame, no una lista
    
    # Filtrar los artículos de tipo "noticia"
    noticias <- noticias[noticias$`_type` == "noticia", ]
    
    # Extraer datos de manera adecuada desde el data.frame
    for (i in 1:nrow(noticias)) {
      temp <- noticias[i, ]
      
      # Verificar que temp tenga los datos necesarios antes de procesar
      if (!is.null(temp$`_source.id`) && !is.null(temp$`_source.texto`) && !is.null(temp$`_source.titulo`) && !is.null(temp$`_source.fechaPublicacion`)) {
        # Extraemos los datos de las columnas adecuadas
        all_data_emol <- rbind(all_data_emol, data.frame(
          ID = as.character(temp$`_source.id`),
          post_content = as.character(temp$`_source.texto`),
          post_title = as.character(temp$`_source.titulo`),
          year = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% year()),
          month = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% month()),
          day = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% day()),
          seccion = as.character(temp$`_source.seccion`),
          subSeccion = as.character(temp$`_source.subSeccion`),
          stringsAsFactors = FALSE
        ))
      }
    }
  }
  
all_data_emol <- all_data_emol %>%
  filter(!is.na(post_content) & post_content != "")
  
  # Mostrar progreso de las iteraciones
  cat(" - Procesados - ", offset, "\n") # Mostrar progreso
}

# Verificar el primer contenido extraído
print(all_data_emol$post_content[1])

total_results_emol <- nrow(all_data_emol)
```

## Referencias

-   EMOL. (2024). En Wikipedia, la enciclopedia libre.<https://es.wikipedia.org/w/index.php?title=EMOL&oldid=160302426>

-   Quiénes somos \| BioBiochile. (s. f.). Recuperado 4 de noviembre de 2024, de<https://www.biobiochile.cl/quienes-somos/>

-   Clasificación de los principales sitios web de Editoriales de noticias y medios de comunicación en Chile en septiembre 2024. (s. f.). Similarweb. Recuperado 4 de noviembre de 2024, de<https://www.similarweb.com/es/top-websites/chile/news-and-media/>
