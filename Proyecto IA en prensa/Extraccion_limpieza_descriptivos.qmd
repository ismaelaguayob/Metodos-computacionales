---
title: "Trabajo 2: Web Scraping e IA"
format: html
editor: visual
echo: true
css: styles.css 
editor_options: 
  chunk_output_type: inline
---

# Segundo trabajo Métodos Computacionales

#### *Integrantes: Ismael Aguayo y Exequiel Trujillo*

## Relevancia de los datos

El uso de técnicas de extracción de datos por medio de APIs permite obtener grandes volúmenes de información de fuentes digitales, lo cual es particularmente valioso para las ciencias sociales, que tradicionalmente se han basado en encuestas y censos que pueden ser más costosos y limitados en tiempo y alcance.

Las técnicas de web scraping permiten explorar temas actuales y en constante cambio (mientras se publican contenidos nuevos a diario). Quienes hacen investigación en ciencias sociales necesitan adaptarse a entornos digitales dinámicos. A través de este proyecto se utilizan herramientas computacionales en la recolección y análisis de datos, con una pretensión exploratoria por una parte y también con el objetivo de desarrollar herramientas específicas para el análisis de discurso en la prensa sobre algún tema determinado. En este caso, el tema que se usa de ejemplo es el de la Inteligencia Artificial.

La relevancia sociológica de los datos que se utilizarán se sostiene, en principio, en el elemento revolucionario y transformador que presenta una tecnología como la IA. Segundo, la utilización de medios masivos de comunicación como BíoBíocl y Emol, ambos los medios online más visitados en el país. Esto vinculado con el gran volumen de datos que se está utilizando, otorga validez y significancia a los análisis sociales que se realizan.

La IA ha permeado el trabajo académico en diversos ámbitos, siendo de los más visitados entre estos desde la sociología, la educación y el trabajo. Sin embargo, se percibe una vacío académico en análisis constructivistas que perciban la tecnología no solo como un objeto técnico, sino uno imbricado socialmente con significados, prácticas, y con un gran impacto en la cultura; uno construido en un proceso simultáneamente técnico como social. En los medios, esta construcción es evidente, y tiene un impacto mayor en los significados construidos por los individuos tanto por su masividad como por su posición privilegiada en la sociedad como emisor de información con validez. 

## Contexto BíoBío y Emol

Para este proyecto, decidimos trabajar con Emol y Bío Bío, dos de los medios de noticias más conocidos en Chile y que tienen un gran impacto en cómo la gente entiende lo que pasa en el país. Nos interesa especialmente porque ambos cubren temas de política, economía, sociedad, tecnología y cultura, en particular sobre Inteligencia Artificial, lo que nos ayuda a explorar cómo se presentan y enmarcan temas importantes o de impacto como este.

Emol es parte del grupo El Mercurio, uno de los conglomerados de medios más grandes de Chile. Este medio tiene una fuerte influencia en la opinión pública y su contenido se enfoca mucho en temas económicos, políticos y empresariales. Creemos que analizar artículos de Emol nos permite ver cómo se representan ciertos sectores de la sociedad y observar el tipo de lenguaje que usan en temas sensibles o de alto interés para la gente. El medio de comunicación El Mercurio On-Line (Emol) es parte de la empresa El Mercurio S.A.P., siendo su portal digital de noticias. Nació a mediados de los 90s con el objetivo de informar a las empresas en economía y actualidad, sin embargo, debido a la competencia de La Tercera online y Terra, se convirtió en lo que es hoy, publicando el contenido de El Mercurio, La segunda y Las Últimas Noticias (Wikipedia, 2024).

Por otro lado, Bío Bío tiene un enfoque más amplio y diverso, pero sigue siendo uno de los medios más consumidos en Chile. El medio BíoBío.cl, del que se extraerán los datos del presente estudio, data de 1959. En principio era una radio, siendo la única con sede fuera de la capital. Para el año 2009 se creó la página web que se utilizará, la cual en 2020 es la página web de noticias más visitada del país, con 122 millones de usuarios únicos y 595 millones de visitas (Quiénes somos \| BioBiochile, s. f.).

Ambos medios son los más visitados digitalmente en el país, estando en las posiciones primera (Emol) y segunda (BioBiocl), según el sitio web de rankings SimilarWeb (Septiembre de 2024). 

```{r}

#Cargamos libreria

library(pacman)

p_load(
  tidyverse,
  httr,
  jsonlite,
  tidytext,
  ggplot2,
  ggvenn,
  rvest,
  stringr,
  xml2,
  wordcloud2,
  arrow,
  stopwords,
  lubridate,
  htmlwidgets
)

rm(list = ls()) 
```

## Obtenemos el texto de las noticias con la búsqueda "Inteligencia Artificial" en medio Bío Bío

Se utilizó la API de búsqueda que utiliza la web de BíoBío.cl. Esta no cuenta con documentación, por lo que a prueba y error se fueron descubriendo los parámetros necesarios para automatizar el scrappeo del contenido de la web. Los headers que se utilizaron son los mismos que arroja el get cuando uno realiza una busqueda desde el navegador.

```{r}

## Parámetros básicos

search_query <- "inteligencia artificial" # Frase a buscar
offset <- 0  # En 0 para que comience por el primer artículo, así funciona la API

# Numero de búsquedas total al ingresar la palabra clave en bíobío.cl (la API busca en múltiplos de a 20)
total_results <- 0

all_data <- data.frame(ID = character(),  # Definimos columnas vacías
                       post_content = character(),
                       post_title = character(),
                       year = integer(),  # Año como entero
                       month = integer(),  # Mes como entero
                       day = integer(),  # Día como entero
                       post_category_primary.name = character(),
                       post_category_secondary.name = character(),
                       stringsAsFactors = FALSE)  # Inicializamos dataframe vacío

# Encabezados para la solicitud
headers <- c(
  `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
  `Accept` = "application/json, text/plain, */*",
  `Referer` = paste0("https://www.biobiochile.cl/buscador.shtml?s=", URLencode(search_query)),
  `Content-Type` = "application/json; charset=UTF-8"
)

# URL de la solicitud inicial
url_initial <- paste0(
  "https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
  "&search=", URLencode(search_query),
  "&intervalo=&orden=ultimas"
)

# Realizar la solicitud
response_initial <- GET(url_initial, add_headers(.headers = headers))

# Procesar la respuesta
if (response_initial$status_code == 200) {
  # Convertir el contenido a texto y luego a JSON
  data_initial <- content(response_initial, "text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE)
  
  # Extraer el total de resultados
  if (!is.null(data_initial$total)) {
    total_results <- as.numeric(data_initial$total)
    message("Número total de resultados disponibles: ", total_results)
  } else {
    stop("No se encontró el parámetro 'total' en la respuesta.")
  }
} else {
  stop("Error al realizar la solicitud inicial. Código de estado: ", response_initial$status_code)
}

## Iteramos hasta que el offset sea menor al total de resultados

while (offset < total_results) {
  # Construimos el link para cada iteración
  url <- paste0(
    "https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
    "&search=", URLencode(search_query),
    "&intervalo=&orden=ultimas"
  )
  
  # Se aumenta el offset para cada iteración (después de construido el link)
  offset <- offset + 20
    
  # Realizamos la solicitud y manejamos posibles errores
  response <- tryCatch(
    { GET(url) },
    error = function(e) { 
      message("Error en la conexión: ", e) 
      return(NULL)
    }
  )
    
  # Verificamos si `response` es nulo antes de continuar
  if (is.null(response)) next
  
  # Procesamos el contenido si `response` no es nulo
  data <- content(response, "text", encoding = "UTF-8")
  json_data <- fromJSON(data, flatten = TRUE)

  # Verificamos que el elemento `notas` existe antes de unir datos
  if (!is.null(json_data$notas)) {
    json_notas <- json_data$notas %>%
      # Verificamos si las columnas existen y les asignamos valores o NA
  mutate(
    post_category_primary.name = ifelse("post_category_primary.name" %in% names(.), 
                                   json_data$notas$post_category_primary.name, NA),
    post_category_secondary.name = ifelse("post_category_secondary.name" %in% names(.), 
                                     json_data$notas$post_category_secondary.name, NA)
  ) %>%
      select(ID,  
             post_content, 
             post_title,  
             year, 
             month,  
             day, 
             post_category_primary.name,  
             post_category_secondary.name) %>%
      mutate(
        across(everything(), as.character),  # Convertimos todo a caracteres primero
        year = as.integer(year),  # Convertimos 'year' a entero
        month = as.integer(month),  # Convertimos 'month' a entero
        day = as.integer(day)  # Convertimos 'day' a entero
      )
    
    # Combinamos los nuevos datos con los datos existentes
    all_data <- bind_rows(all_data, json_notas)
  }
  
  # Opcional
  #cat(" - Procesados - ", offset, "\n") # Mostrar progreso
}

# Opcional
# print(all_data$post_content[1]) #Ejemplo de cómo nos queda el contenido de la página


```

## Extraemos los párrafos que mencionan inteligencia artificial o IA

```{r}

# Crear vector con sinónimos en caso de que sea necesario
sinonimos_search_query <- c("IA", "AI")

# (?i): Activamos la insensibilidad a mayúsculas y minúsculas
# \\b: Aseguramos que las palabras coincidan completas, evitando falsos positivos en palabras como "parcial" que contiene "IA".
pattern <- paste0("(?i)\\b(", search_query, "|", paste(sinonimos_search_query, collapse = "|"), ")\\b")

# Expresión regular optimizada para buscar la search_query o sinónimos
all_data <- all_data %>%
  mutate(
    parrafos_filtrados = map(post_content, ~ {
      
      # (.x): Seleccionamos el elemento actual de post_content dentro de la iteración
      # Con try catch para que sea NA en caso de error y no detenga la iteración
      nodo_html <- tryCatch(read_html(.x), error = function(e) return(NA))
      
      # Verificamos que el nodo HTML fue leído correctamente.
      if (!is.na(nodo_html)) {
        parrafos <- nodo_html %>% html_elements("p") %>% html_text2()
        parrafos[grepl(pattern, parrafos)]
      } else {
        NA # En caso de que hubo un error al leer el html
      }
    })
  )

print(all_data$parrafos_filtrados[1])
```

## Limpieza de las secciones "Lee también...", Instagram y Twitter

Aparecen algunos bloques html que ponen noticias como recomedación para leer y pueden contener las palabras "inteligencia artificial" sin que el contenido de la nota se refiera específicamente a eso. Para ello podemos usar el paquete rvest. Lo mismo haremos con los bloques que tienen publicaciones de X (ex Twitter) o de Instagram. Así, mantendremos solo el texto de la noticia y eliminaremos las noticias que se agregaron sin que sean relevantes.

#### Necesitamos una función que se dedique a encontrar esa parte del texto y eliminarla:

```{r}

for (i in seq_len(nrow(all_data))){
  # Convertimos el contenido a un objeto HTML para usar rvest
  contenido_html <- rvest::read_html(all_data$post_content[[i]])

  # Eliminamos los divs con la clase 'lee-tambien-bbcl'
  contenido_html %>%
    html_nodes("div.lee-tambien-bbcl") %>%
    xml_remove()

  # Eliminamos los divs de publicaciones de Instagram
  contenido_html %>%
    html_nodes("blockquote.instagram-media") %>%
    xml_remove()

  # Eliminamos los divs de publicaciones de Twitter
  contenido_html %>%
    html_nodes("blockquote.twitter-tweet") %>%
    xml_remove()

  # Convertimos el HTML limpio a texto y eliminamos cualquier "Lee también" que quede en el contenido
  contenido_texto <- as.character(contenido_html)
  contenido_texto <- str_replace_all(contenido_texto, regex("Lee también.*?<\\/div>", dotall = TRUE), "")
  
  # Guardamos el contenido limpio de vuelta en el data frame
  all_data$post_content[[i]] <- contenido_texto

  #Revisamos el contenido HTML resultante (opcional)
  # print(as.character(contenido_html))
}

```

#### Ahora eliminamos todas las notas que no mencionen Inteligencia Artificial (y sinónimos) por lo menos una vez:

```{r}

# Identificamos las filas que NO contienen la búsqueda o sus sinónimos
indices_no_match <- which(!grepl(pattern, all_data$post_content, ignore.case = TRUE))

# Test
# print(all_data$post_content[indices_no_match[1]])

# Eliminamos las filas que no contienen el término de búsqueda usando los índices
all_data <- all_data[-indices_no_match, ]

# Actualizamos "total_results" con el número total de filas en el data frame final
total_results <- nrow(all_data)

# Confirmación de la actualización
cat("El número total de resultados obtenidos es:", total_results, "\n")

# Confirmación de las filas eliminadas
# cat("Los índices de las filas eliminadas son:", indices_no_match, "\n")
```

#### También podemos exportar como html para ver si desaparecieron los bloques completos

```{r}

# Seteamos nombre archivo salida
out_file <- "noticia_casi.html"

# Exportamos el contenido HTML de una nota aleatoria
writeLines(as.character(all_data$post_content[sample(1:total_results, 1)]), con = out_file)

# Mensaje de confirmación 
cat("El contenido en bruto se ha exportado a", out_file) 
```

## Limpieza de texto general

Eliminamos la parte de código que queda en el texto para convertirlo en texto plano.

```{r}
# Inicializamos variables
contador <- 1
html_content <- list()       # Lista para hacer nodos html

# Procesar el HTML y extraer el texto
while (contador <= total_results) {
  # Convertir a nodo HTML
  html_content[[contador]] <- read_html(all_data$post_content[[contador]])
  
  # Extraemos y limpiamos el texto
  all_data$post_content[[contador]] <- html_content[[contador]] %>%
    html_text2() %>% 
    str_squish()
  
  contador <- contador + 1
}

# Ejemplo del contenido de una nota en plano ahora
print(all_data$post_content[sample(1:total_results, 1)])
```

## Descriptivos de frecuencia de palabras

```{r}

# Seleccionamos solo la columna de texto que nos interesa
text_data <- all_data %>% select(post_content)

# Tokenizamos el texto y lo dividimos en palabras
words <- all_data %>%
  unnest_tokens(word, post_content)

# Cantidad de palabras extraídas

nrow(words)

# Cargar palabras comunes en inglés y español
data("stop_words") # Cargar palabras comunes en inglés (desde tidytext)
stop_words_es_comunes <- stopwords("es",source ="snowball")
stop_words_es_comunes <- tibble(word = stop_words_es_comunes)

# Si es necesario, crear un tibble con palabras que particularmente para un search_query específico no interesan
stop_words_particulares <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "lee", "las", "para", "se", "es", "su",  "del", "una", "al", "como", "más", "lo", "este", "sus", "esta", "también", "entre", "fue", "han", "un", "sin", "sobre", "ya", "pero", "no", "muy", "si", "porque", "cuando", "desde", "todo", "son", "ha", "hay", "le", "ni", "cada", "me", "tanto", "hasta", "nos", "mi", "tus", "mis", "tengo", "tienes", "esa", "ese", "tan", "esa", "esos", "esa", "esas", "él", "ella", "ellos", "ellas", "nosotros", "vosotros", "vosotras", "ustedes", "uno", "una", "unos", "unas", "alguien", "quien", "cual", "cuales", "cualquier", "cualesquiera", "como", "donde", "cuanto", "demasiado", "poco", "menos", "casi", "algunos", "algunas", "aunque", "cuyo", "cuya", "cuyos", "cuyas", "ser", "haber", "estar", "tener", "hacer", "ir", "ver", "dar", "debe", "debido", "puede", "pues", "dicho", "hecho", "mientras", "luego", "además", "entonces", "así", "tal", "dicha", "mismo", "misma", "demás", "otro", "otra", "otros", "otras", "debería", "tendría", "podría", "menos", "cuándo", "dónde",  "qué", "quién", "cuyo", "la", "lo", "las", "que", "está", "según", "esto", "inteligencia", "artificial", "ia", "tecnología", "chile", "años", "personas", "parte", "tiene", "año", "cómo", "están", "forma", "durante", "vez", "estos", "pueden", "todos", "eso", "dos", "través", "hace", "solo", "gran", "estas", "ahora", "manera", "dijo", "cuenta", "ejemplo", "hoy", "bien", "día", "incluso", "mayor", "mejor", "embargo", "mucho", "era", "primera", "caso", "nuevas", "sido", "tipo", "nuestro", "sino", "antes", "tras", "te", "tienen", "junto", "será", "pasado", "momento", "primer", "grandes", "crear", "trata", "algo", "sólo", "todas", "nuestra", "después", "contra", "nueva", "nuevo", "espacio", "permite", "quienes", "sí", "sea", "tres", "estamos", "lugar", "aún", "nuevos", "respecto", "medio", "muchos", "horas", "mil", "nivel", "días", "persona", "ello", "gracias", "centro", "10", "grupo", "tu", "siempre", "2", "real", "realidad", "había", "5", "12", "2023", "2021", "muchas", "va", "1", "6", "7", "4", "3", "8", "9", "0"))


# Filtramos las stop words del texto
words_clean <- words %>%
  anti_join(stop_words, by = "word") %>%
  anti_join(stop_words_es_comunes, by = "word") %>%
  anti_join(stop_words_particulares, by = "word")

# Calculamos frecuencia de palabras
word_counts <- words_clean %>%
  count(word, sort = TRUE)

# Ver las 10 palabras más frecuentes
head(word_counts, 10)

# Realizamos nube de palabras, más frecuentes

word_counts_filtered <- word_counts %>% filter(n > 200) %>% slice_max(n, n = 70)

wordcloud2(
  data = word_counts_filtered, 
  size = 0.3,              # Aumenta el tamaño general de las palabras
  minSize = 0,             # Asegura que todas las palabras sean visibles
  gridSize = 1,            # Ajusta la densidad de palabras
  color = "random-dark",   # Colores para las palabras
  backgroundColor = "white", # Fondo blanco
  shape = "circle",        # Forma circular para compactar la nube
  ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
```

## Descriptivo cantidad de notas por meses

```{r}

# Agrupamos los datos por año y mes, y contar las publicaciones
publicaciones_por_mes <- all_data %>%
  group_by(year, month) %>%
  summarise(cantidad = n(), .groups = 'drop') %>%
  mutate(fecha = as.Date(paste(year, month, "01", sep = "-"))) # Crear una fecha para el eje x

# Crear título dinámico usando search_query
titulo_grafico <- paste("Cantidad de Notas sobre", search_query, "publicadas por mes")

# Creamos el gráfico
ggplot(publicaciones_por_mes, aes(x = fecha, y = cantidad)) +
  geom_line(color = "blue", linewidth = 1) + # Línea de publicaciones
  geom_point(color = "red", size = 1) + # Puntos en cada mes
  geom_smooth(method = "loess", color = "green", se = FALSE, linewidth = 1) + #   Curva de tendencia
  labs(title = titulo_grafico, # Usamos el título dinámico
       x = "Año",
       y = "Cantidad de Notas") +
  theme_minimal() +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") # Mostrar solo los años en el eje x
```

Se visualiza un aumento exponencial de noticias que se condice con el boom de la inteligencia artificial (IA) a fines del 2022 por la salida de ChatGPT. Luego hay picos altos y bajos, sin embargo nunca se vuelve a la frecuencia antes del boom inicial. Podría ser interesante investigar las fechas de los picos altos para ver si ocurrieron hechos importantes en relación a la IA.

## Descriptivo cantidad de notas por categoría

```{r}

# Contamos las publicaciones por categoría
publicaciones_por_categoria <- all_data %>%
  group_by(post_category_primary.name) %>%
  summarise(cantidad = n(), .groups = 'drop') %>%
  arrange(desc(cantidad))

# Filtramos para quedarnos con las 15 categorías más usadas
top_15_categorias <- publicaciones_por_categoria %>%
  top_n(15, cantidad)

# Gráfico de Barras para las 15 categorías más usadas
ggplot(top_15_categorias, aes(x = reorder(post_category_primary.name, -cantidad), y = cantidad, fill = post_category_primary.name)) +
  geom_bar(stat = "identity") +
  labs(title = paste("15 categorías más usadas en notas de Bío-Bío sobre", search_query),
       x = "Categoría",
       y = "Cantidad de Notas") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), 
        legend.position = "none")
```

Como era de esperarse, el tema más recurrente es Ciencia y Tecnología, seguido de Economía, Nacional, Artes y Cultura y Opinión. Sin embargo, notamos que las categorías primaria y secundaria son puestas durante ciertos periodos de tiempo (algunos días) sin tener relación necesariamente con los tópicos importantes en las notas.

## Guardamos el data frame procesado en formato parquet

```{r}
arrow::write_parquet(all_data, "data.parquet")
```

# Procesamiento luego del análisis de sentimiento en cada párrafo filtrado

## Importamos el parquet actualizado

Luego de procesar en el notebook de Google Colab, importamos los resultados con análisis de sentimiento. Necesitamos también restablecer la columna de parrafos_filtrados que en el parquet exportado desde el Colab no lo guarda como una lista de párrafos.

```{r}
# Leemos ambos archivos Parquet
all_data <- read_parquet("data.parquet")

# Leer el archivo Parquet exportado
all_data_updated <- read_parquet("all_data_updated.parquet")

# Reemplazamos la columna
all_data_updated$parrafos_filtrados <- all_data$parrafos_filtrados
```

## Procesamos el nuevo data frame

Además en este punto debemos configurar un umbral desde el cual se considerarán válidas las predicciones de sentimiento (0.9 implicaría supuestamente un 90% de probabilidades de que la predicción esté bien).

```{r}
# Copiamos la columna 'parrafos_filtrados' de all_data a all_data_updated
all_data_updated$parrafos_filtrados <- all_data$parrafos_filtrados

# Definimos un umbral para considerar las valoraciones positivas o negativas
umbral_sentimiento <- 0.90

# Calculamos el sentimiento predominante basado en los puntajes y el umbral
all_data_updated <- all_data_updated %>%
  mutate(
    predominant_sentiment = sapply(sentiment, function(x) {
      # Filtramos y calculamos los puntajes de sentimientos positivos y negativos que superan el umbral
      positive_score <- sum(ifelse(x$label == "Positive" & x$score > umbral_sentimiento, x$score, 0))
      negative_score <- sum(ifelse(x$label == "Negative" & x$score > umbral_sentimiento, x$score, 0))

      # Determinamos el sentimiento predominante basado en la suma de los puntajes
      if (positive_score > negative_score) {
        return("Positive")
      } else if (negative_score > positive_score) {
        return("Negative")
      } else {
        return("Neutral")  # En caso de empate o cuando ninguno supera el umbral
      }
    })
  )

# Convertimos la columna sentiment a un data frame
all_data_updated$sentiment <- as.list(all_data_updated$sentiment)

```

## Descriptivos con los nuevos datos

```{r}

# Filtramos las filas que son "Positive" o "Negative" (eliminando "Neutral")
filtered_data <- all_data_updated %>%
  filter(predominant_sentiment %in% c("Positive", "Negative"))

# Contamos las notas por mes y sentimiento
monthly_sentiment_count <- filtered_data %>%
  mutate(fecha = as.Date(paste(year, month, "01", sep = "-"))) %>%  # Creamos una fecha para el eje x
  group_by(fecha, predominant_sentiment) %>%
  summarise(note_count = n(), .groups = "drop")

# Crear título dinámico usando search_query
titulo_grafico <- paste("Cantidad de Notas sobre", search_query, "por Año y Sentimiento Predominante")

# Graficamos
ggplot(monthly_sentiment_count, aes(x = fecha, y = note_count, color = predominant_sentiment, group = predominant_sentiment)) +
  geom_line(linewidth = 1, alpha = 0.4) +  # Líneas con baja opacidad
  geom_smooth(method = "loess", aes(color = predominant_sentiment), se = FALSE, size = 1.5) +  # Línea de tendencia más visible
  labs(
    title = titulo_grafico,
    x = "Año",
    y = "Cantidad de Notas",
    color = "Sentimiento Predominante"
  ) +
  theme_minimal() +
  scale_x_date(date_labels = "%Y", date_breaks = "1 year") +  # Mostrar solo los años en el eje x
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Mejor presentación del eje x

```

Se puede apreciar en un primer momento -más claramente desde 2015 en adelante- que el sentimiento predominante positivo "despega" primero y la pendiente crece más rápido que el negativo, que tiene un leve aumento el año 2019 pero despega definitivamente el año 2022 (coincidente con la liberación de Chat GPT) aunque con una curva menos pronunciada que las positivas, cuyo crecimiento tiene una tendencia de aceleración mayor.

# Wordclouds por sentimiento

## Creamos conjuntos de palabras más frecuentes en positivas y negativas

```{r}

# Crear listas para almacenar las palabras
palabras_positivas <- c()
palabras_negativas <- c()

# Por cada caso de noticia
for (i in 1:nrow(all_data_updated)) {
  
  # Aseguramos que 'sentiment' tiene datos para el caso
  if (length(all_data_updated$sentiment[[i]]) > 0) {
    # Extraemos los datos de label y score
    label_lst_tmp <- all_data_updated$sentiment[[i]][[1]]
    score_lst_tmp <- all_data_updated$sentiment[[i]][[2]]
    
    # Verificamos que score_lst_tmp tenga datos válidos
    if (length(score_lst_tmp) > 0) {
      # Recorremos score_lst_tmp usando length() si es un vector
      for(j in 1:length(score_lst_tmp)){
        # Ver si es mayor que el umbral
        if (score_lst_tmp[[j]] > umbral_sentimiento){
          # Crear un tibble temporal con el texto del párrafo
          temp_tibble <- tibble(text = all_data_updated$parrafos_filtrados[[i]][[j]])
          
          # Tokenizamos usando unnest_tokens
          palabras <- temp_tibble %>%
            unnest_tokens(word, text) %>%
            pull(word)  # Extraer las palabras como vector
          
          # Agregar las palabras a la lista de palabras según corresponda
          if (label_lst_tmp[[j]] == "Positive") {
            palabras_positivas <- c(palabras_positivas, palabras)
          } else if (label_lst_tmp[[j]] == "Negative") {
            palabras_negativas <- c(palabras_negativas, palabras)
          }
        } else {
          # Es neutral
        } 
      }
    }
  }
}

# Convertir las listas de palabras en dataframes/tibbles
palabras_positivas_df <- tibble(word = palabras_positivas)
palabras_negativas_df <- tibble(word = palabras_negativas)


# Eliminar stop words de palabras positivas y negativas
palabras_positivas_sin_stop <- palabras_positivas_df %>%
  anti_join(stop_words, by = "word") %>%
  anti_join(stop_words_es_comunes, by = "word") %>%
  anti_join(stop_words_particulares, by = "word")

palabras_negativas_sin_stop <- palabras_negativas_df %>%
  anti_join(stop_words, by = "word") %>%
  anti_join(stop_words_es_comunes, by = "word") %>%
  anti_join(stop_words_particulares, by = "word")

# Convertir las listas de palabras en dataframes/tibbles
palabras_positivas_df <- tibble(word = palabras_positivas_sin_stop)
palabras_negativas_df <- tibble(word = palabras_negativas_sin_stop)
```

```{r}
# Contar las palabras más frecuentes en palabras positivas
palabras_positivas_frecuentes <- palabras_positivas_df %>%
  count(word$word, sort = TRUE)
# Renombrar la columna 'word$word' a 'word'
palabras_positivas_frecuentes <- palabras_positivas_frecuentes %>%
  rename(word = `word$word`)

pos_freq_top <- palabras_positivas_frecuentes %>% 
  #filter(n > 100) %>% # Esta es por si queremos poner un mínimo de veces
  slice_max(n, n = 70) # Cantidad de resultados a mostrar

wordcloud2(
  data = as.data.frame(pos_freq_top), 
  size = 0.3,              # Aumenta el tamaño general de las palabras
  minSize = 0,             # Asegura que todas las palabras sean visibles
  gridSize = 1,            # Ajusta la densidad de palabras
  color = "random-dark",   # Colores para las palabras
  backgroundColor = "white", # Fondo blanco
  shape = "circle",        # Forma circular para compactar la nube
  ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)

```

```{r}

# Contar las palabras más frecuentes en palabras negativas
palabras_negativas_frecuentes <- palabras_negativas_df %>%
  count(word$word, sort = TRUE)
# Renombrar la columna 'word$word' a 'word'
palabras_negativas_frecuentes <- palabras_negativas_frecuentes %>%
  rename(word = `word$word`)

neg_freq_top <- palabras_negativas_frecuentes %>% 
  #filter(n > 100) %>% # Esta es por si queremos poner un mínimo de veces
  slice_max(n, n = 70) # Cantidad de resultados a mostrar

wordcloud2(
  data = as.data.frame(neg_freq_top), 
  size = 0.3,              # Aumenta el tamaño general de las palabras
  minSize = 0,             # Asegura que todas las palabras sean visibles
  gridSize = 1,            # Ajusta la densidad de palabras
  color = "random-dark",   # Colores para las palabras
  backgroundColor = "white", # Fondo blanco
  shape = "circle",        # Forma circular para compactar la nube
  ellipticity = 1          # Elimina la elipse y fuerza un formato más centrado
)
```

## Creamos conjuntos de intersecciones

```{r}

# Crear los tres conjuntos

# 1. solo_positivas: Mantener las frecuencias de las palabras solo positivas
solo_positivas <- palabras_positivas_frecuentes %>%
  filter(!word %in% palabras_negativas_frecuentes$word)

# 2. solo_negativas: Mantener las frecuencias de las palabras solo negativas
solo_negativas <- palabras_negativas_frecuentes %>%
  filter(!word %in% palabras_positivas_frecuentes$word)

# 3. compartidas: Palabras que están en ambas listas (positivas y negativas)

# Realizar el inner join para obtener las palabras compartidas
palabras_compartidas <- inner_join(palabras_positivas_frecuentes, palabras_negativas_frecuentes, by = "word")

# Sumar las frecuencias de las palabras compartidas
palabras_compartidas <- palabras_compartidas %>%
  mutate(n = n.x + n.y) %>%  # Sumar las frecuencias
  select(word, n)   # Mantener solo las columnas 'word' y 'n'

# Convertir las columnas 'word' en vectores de palabras para el gráfico de Venn
palabras_positivas_set <- palabras_positivas_frecuentes$word
palabras_negativas_set <- palabras_negativas_frecuentes$word

# Crear lista para el gráfico de Venn
word_sets <- list(
  Positive = palabras_positivas_set,
  Negative = palabras_negativas_set
)

# Crear gráfico de Venn
ggvenn(
  word_sets, 
  fill_color = c("#FFA07A", "#87CEEB"),  # Colores diferenciados
  stroke_size = 0.5,                    # Grosor de las líneas
  set_name_size = 5                     # Tamaño de los nombres
)

# Opcional: Si deseas ver los conjuntos calculados
print(solo_positivas)
print(solo_negativas)
print(palabras_compartidas)

```

# Web-scrapping utilizando la API de Emol

Lo que sigue de aquí en adelante se encuentra incompleto, sin embargo, se consideró relevante incluirlo en la entrega para poder recibir una retroalimentación. El procedimiento es el mismo: se encontró una API no documentada que utiliza el buscador de Emol. Luego se fueron descubriendo a prueba y error los parámetros necesarios para iterar. La mayor complicación que se tuvo es que la estructura de datos que entrega la API difiere mucho en este caso. La API entrega las noticias agrupadas de diez en diez.

```{r}

# Parámetros básicos
search_query <- "Inteligencia Artificial"  # Palabra clave para obtener los artículos
offset <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results_emol <- 0  # Número total de resultados de la búsqueda

# Definir URL base y parámetros de la consulta
base_url <- "https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes"
size <- 10  # Número de artículos por página

# Inicializar el data frame vacío
all_data_emol <- data.frame(
  ID = character(),  # Definimos las columnas vacías
  post_content = character(),
  post_title = character(),
  year = integer(),  # Año como entero
  month = integer(),  # Mes como entero
  day = integer(),  # Día como entero
  seccion = character(),
  subSeccion = character(),
  stringsAsFactors = FALSE  # Inicializamos el data frame sin factores
)

# Obtener el número total de resultados
url_initial <- paste0(
  base_url, 
  "?q=", URLencode(search_query), 
  "&size=1&from=0"
)

response_initial <- GET(url_initial)
if (status_code(response_initial) == 200) {
  data_initial <- content(response_initial, "text", encoding = "UTF-8") %>%
    fromJSON(flatten = TRUE)
  
  if (!is.null(data_initial$hits$total)) {
    total_results_emol <- as.numeric(data_initial$hits$total)
    message("Número total de resultados: ", total_results_emol)
  } else {
    stop("No se encontró el parámetro 'total' en la respuesta.")
  }
} else {
  stop("Error en la solicitud inicial. Código de estado: ", status_code(response_initial))
}

# Iterar por todas las páginas de resultados
while (offset < total_results_emol) {
  # Construir la URL para cada iteración
  url <- paste0(
    base_url, 
    "?q=", URLencode(search_query), 
    "&size=", size, 
    "&from=", offset
  )
  
  # Aumentar el offset para la siguiente iteración
  offset <- offset + size
  
  # Realizar la solicitud y manejar posibles errores
  response <- tryCatch(
    { GET(url) },
    error = function(e) { 
      message("Error en la conexión: ", e) 
      return(NULL)
    }
  )
  
  if (is.null(response)) next
  
  # Procesar el contenido si la respuesta es válida
  data <- content(response, "text", encoding = "UTF-8")
  json_data <- fromJSON(data, flatten = TRUE)
  
  # Verificar si hay datos en `hits$hits` antes de procesar
  if (!is.null(json_data$hits$hits)) {
    noticias <- json_data$hits$hits  # Esto es un data.frame, no una lista
    
    # Filtrar los artículos de tipo "noticia"
    noticias <- noticias[noticias$`_type` == "noticia", ]
    
    # Extraer datos de manera adecuada desde el data.frame
    for (i in 1:nrow(noticias)) {
      temp <- noticias[i, ]
      
      # Verificar que temp tenga los datos necesarios antes de procesar
      if (!is.null(temp$`_source.id`) && !is.null(temp$`_source.texto`) && !is.null(temp$`_source.titulo`) && !is.null(temp$`_source.fechaPublicacion`)) {
        # Extraemos los datos de las columnas adecuadas
        all_data_emol <- rbind(all_data_emol, data.frame(
          ID = as.character(temp$`_source.id`),
          post_content = as.character(temp$`_source.texto`),
          post_title = as.character(temp$`_source.titulo`),
          year = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% year()),
          month = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% month()),
          day = as.integer(ymd_hms(temp$`_source.fechaPublicacion`) %>% day()),
          seccion = as.character(temp$`_source.seccion`),
          subSeccion = as.character(temp$`_source.subSeccion`),
          stringsAsFactors = FALSE
        ))
      }
    }
  }
  
all_data_emol <- all_data_emol %>%
  filter(!is.na(post_content) & post_content != "")
  
  # Mostrar progreso de las iteraciones
  # cat(" - Procesados - ", offset, "\n") # Mostrar progreso
}

# Verificar el primer contenido extraído
print(all_data_emol$post_content[160])

# Con expresión regular sacar todo el html resultante dentro del texto
# 

total_results_emol <- nrow(all_data_emol)
```

## Referencias

-   EMOL. (2024). En Wikipedia, la enciclopedia libre.<https://es.wikipedia.org/w/index.php?title=EMOL&oldid=160302426>

-   Quiénes somos \| BioBiochile. (s. f.). Recuperado 4 de noviembre de 2024, de<https://www.biobiochile.cl/quienes-somos/>

-   Clasificación de los principales sitios web de Editoriales de noticias y medios de comunicación en Chile en septiembre 2024. (s. f.). Similarweb. Recuperado 4 de noviembre de 2024, de<https://www.similarweb.com/es/top-websites/chile/news-and-media/>

## To do list para crear Paquete:

-   Función que permita devolver los n últimas noticias (ordenadas desde las más recientes)

-   Investigar parámetros de la API para ver utilizar rangos de fecha

-   Una vez obteniendo el contenido, realizar funciones para analizar el contenido

-   Proponer una función y sus parámetros
