---
title: "Tarea 1: scrapping"
format: html
editor: visual
---

## Web-Scrapping

El siguiente documento dinámico presentará el proceso de web-scapping que se realizó para el ramo "Métodos Computacionales para las Ciencias Sociales".

```{r}

#Cargamos libreria

library(pacman)

p_load(
  tidyverse,
  httr,
  jsonlite,
  dplyr
)

rm(list = ls()) 
```

```{r}

## Parámetros básicos
search_query <- "inteligencia artificial" # Palabra clave para obtener los artículos
offset <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2132  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido

# Encabezados para la solicitud
headers <- c(
  `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
  `Accept` = "application/json, text/plain, */*",
  `Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
  `Content-Type` = "application/json; charset=UTF-8"
)

## Iteramos hasta que el offset sea menor al total de resultados

while (offset < total_results) {
  # Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
  url <- paste0(
    "https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
    "&search=", URLencode(search_query),
    "&intervalo=&orden=ultimas"
  )
    offset <- offset + 20 # Se aumenta el offset para cada iteración (después de que se construya el link)
  response <- GET(url) # Realizamos la solicitud
    data <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8 
    json_data <- fromJSON(data, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
    json_notas <- c(json_data$notas) # Creamos un data frame donde solo esté la información relevante
    all_data <- bind_rows(all_data, json_notas)
      cat("Procesados", dim(all_data)) #Mostrar progreso
}


```
