---
title: "Tarea 1: scrapping"
format: html
editor: visual
eval: false
echo: true
---

## Web-Scrapping

El siguiente documento dinámico presentará el proceso de web-scapping que se realizó para el ramo "Métodos Computacionales para las Ciencias Sociales".

```{r}

#Cargamos libreria

library(pacman)

p_load(
  tidyverse,
  httr,
  jsonlite,
  dplyr,
  tidytext,
  ggplot2
)

rm(list = ls()) 
```

## Obtenemos el texto de las noticias con la búsqueda "Inteligencia Artificial" en medio Bío Bío

```{r}

## Parámetros básicos
search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
offset <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2020  # Numero de búsquedas total al ingresar la palabra clave en bíobío.cl
all_data <- data.frame() # Creamos data.frame vacío para almacenar todo el contenido

# Encabezados para la solicitud
headers <- c(
  `User-Agent` = "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
  `Accept` = "application/json, text/plain, */*",
  `Referer` = "https://www.biobiochile.cl/buscador.shtml?s=inteligencia+artificial",
  `Content-Type` = "application/json; charset=UTF-8"
)

## Iteramos hasta que el offset sea menor al total de resultados

while (offset < total_results) {
  # Construimos el link para cada iteración (lo más importante es que el offset vaya aumentando)
  url <- paste0(
    "https://www.biobiochile.cl/lista/api/buscador?offset=", offset,
    "&search=", URLencode(search_query),
    "&intervalo=&orden=ultimas"
  )
    offset <- offset + 20 # Se aumenta el offset para cada iteración (después de que se construya el link)
  response <- GET(url) # Realizamos la solicitud
    data <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8 
    json_data <- fromJSON(data, flatten = TRUE) #Convertimos el JSON en una lista leíble en R
    json_notas <- c(json_data$notas) # Creamos un data frame donde solo esté la información relevante
    all_data <- bind_rows(all_data, json_notas)
      cat("Procesados", dim(all_data)) #Mostrar progreso
}

print(all_data$post_content[1])

datos_proc <- all_data %>% 
  select(
    post_content, ID, post_title, year, month, day, post_category_primary.name, post_category_secondary.name
    )


```

## Repetimos el procedimiento con medio "Emol"

```{r}

## Parámetros básicos

search_query <- "Inteligencia Artificial" # Palabra clave para obtener los artículos
from <- 0  # Seteamos en 0 para que comience por el primer artículo
total_results <- 2000  # Numero de búsquedas total al ingresar la palabra clave en emol.cl
procesados <- 0
all_data_emol <- list() # Creamos lista vacía para almacenar todo el contenido (por el formato del JSON no se puede utilizar un data frame)

combine_lists <- function(...) {
  combined_list <- c(...)
  return(combined_list)
} #Creamos función para combinar listas

## Iteramos hasta que el from sea menor al total de resultados
while (from < total_results) {
  # Construimos el link para cada iteración (lo más importante es que el from vaya aumentando)
  url <- paste0(
    "https://newsapi.ecn.cl/NewsApi/emol/buscador/emol,inversiones,mediosregionales,legal,campo,blogs,guioteca,elmercurio-digital,emoltv,lasegundaprint,revistalibros,mercuriodeportes?q=inteligencia%20artificial&size=10&from=", from
  )
    from <- from + 10 # Se aumenta el from para cada iteración (después de que se construya el link)
  response <- GET(url) # Realizamos la solicitud
    data1 <- content(response, "text", encoding = "UTF-8") # Transformamos el cuerpo de "response" en texto en formato UTF-8 
    json_data1 <- fromJSON(data1, flatten = FALSE) #Convertimos el JSON en una lista leíble en R
    data2 <- list(json_data1[["hits"]][["hits"]][["_source"]][["texto"]]) #Seleccionamos solo el texto para construir una lista
      all_data_emol <- combine_lists(all_data_emol, data2) # Fucionamos todas las listas
          procesados <- procesados +10 # Contador
          print(procesados)
}

textos_df <- tibble(texto = unlist(all_data_emol), na.omit=TRUE) #Convertimos la lista en data frame


```

## Limpieza de texto

```{r}


```

## Descriptivos de frecuencia de palabras

```{r}

##Bio Bio

# Seleccionamos solo la columna de texto que nos interesa
text_data <- all_data %>% select(post_content)

# Tokenizamos el texto y lo dividimos en palabras
words <- text_data %>%
  unnest_tokens(word, post_content)

# Cargar palabras comunes en español
data("stop_words")
stop_words_es <- tibble(word = c("el", "la", "de", "y", "en", "que", "a", "los", "con", "por", "div", "strong", "lee", "class", "https", "8221", "8220", "wp", "style", "las", "para", "se", "es", "href", "su", "del", "una", "es"))

# Filtrar las stop words del texto
words_clean <- words %>%
  anti_join(stop_words, by = "word") %>%
  anti_join(stop_words_es, by = "word")

# Calcular frecuencia de palabras
word_counts <- words_clean %>%
  count(word, sort = TRUE)

# Ver las 10 palabras más frecuentes
head(word_counts, 10)

# Graficar las palabras más frecuentes
word_counts %>%
  filter(n > 10) %>%
  top_n(15, n) %>%
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_col() +
  coord_flip() +
  labs(title = "Frecuencia de Palabras en Artículos",
       x = "Palabra", y = "Frecuencia") +
  theme_minimal()


##Emol

# Paso 2: Tokenizar las palabras y contar frecuencias
frecuencia_palabras <- textos_df %>%
  unnest_tokens(palabra, texto) %>%       # Divide el texto en palabras
  count(palabra, sort = TRUE) 



frecuencia_palabras <- frecuencia_palabras %>%
  filter(!palabra %in% palabras_irrelevantes)

# Paso 4: Graficar las frecuencias de las palabras más comunes
ggplot(frecuencia_palabras %>% top_n(10, n), aes(x = reorder(palabra, n), y = n)) +
  geom_col(fill = "skyblue") +
  coord_flip() +
  labs(title = "Frecuencia de Palabras en Textos",
       x = "Palabra",
       y = "Frecuencia") +
  theme_minimal()



```
